{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from NNS import NeuralNetwork as NN #import neural net code from github to reduce copy/pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.0986249219226374\n",
      "acc: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "class Optimizer_SGD:\n",
    "    \n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1.0 is set as a defaults\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    # Update Parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "\n",
    "#One pass\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = NN.Layer_Dense(2,64)\n",
    "activation1 = NN.Activation_ReLU()\n",
    "dense2 = NN.Layer_Dense(64,3)\n",
    "loss_activation = NN.Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "print('loss: ', loss)\n",
    "\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "if len (y.shape) == 2 :\n",
    "    y = np.argmax(y, axis = 1 )\n",
    "accuracy = np.mean(predictions == y)\n",
    "print( 'acc:' , accuracy)\n",
    "\n",
    "#Backward Pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "#Update weights and biases\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.370, loss: 1.099\n",
      "epoch: 100 , acc: 0.457, loss: 1.075\n",
      "epoch: 200 , acc: 0.457, loss: 1.066\n",
      "epoch: 300 , acc: 0.440, loss: 1.062\n",
      "epoch: 400 , acc: 0.453, loss: 1.060\n",
      "epoch: 500 , acc: 0.463, loss: 1.057\n",
      "epoch: 600 , acc: 0.467, loss: 1.054\n",
      "epoch: 700 , acc: 0.450, loss: 1.050\n",
      "epoch: 800 , acc: 0.410, loss: 1.054\n",
      "epoch: 900 , acc: 0.397, loss: 1.051\n",
      "epoch: 1000 , acc: 0.393, loss: 1.046\n",
      "epoch: 1100 , acc: 0.400, loss: 1.051\n",
      "epoch: 1200 , acc: 0.393, loss: 1.069\n",
      "epoch: 1300 , acc: 0.400, loss: 1.062\n",
      "epoch: 1400 , acc: 0.410, loss: 1.063\n",
      "epoch: 1500 , acc: 0.400, loss: 1.054\n",
      "epoch: 1600 , acc: 0.407, loss: 1.058\n",
      "epoch: 1700 , acc: 0.417, loss: 1.039\n",
      "epoch: 1800 , acc: 0.410, loss: 1.041\n",
      "epoch: 1900 , acc: 0.413, loss: 1.051\n",
      "epoch: 2000 , acc: 0.400, loss: 1.072\n",
      "epoch: 2100 , acc: 0.397, loss: 1.072\n",
      "epoch: 2200 , acc: 0.417, loss: 1.049\n",
      "epoch: 2300 , acc: 0.420, loss: 1.005\n",
      "epoch: 2400 , acc: 0.440, loss: 0.999\n",
      "epoch: 2500 , acc: 0.460, loss: 0.984\n",
      "epoch: 2600 , acc: 0.450, loss: 0.992\n",
      "epoch: 2700 , acc: 0.453, loss: 0.981\n",
      "epoch: 2800 , acc: 0.467, loss: 0.965\n",
      "epoch: 2900 , acc: 0.467, loss: 0.965\n",
      "epoch: 3000 , acc: 0.470, loss: 0.963\n",
      "epoch: 3100 , acc: 0.467, loss: 0.961\n",
      "epoch: 3200 , acc: 0.467, loss: 0.954\n",
      "epoch: 3300 , acc: 0.480, loss: 0.954\n",
      "epoch: 3400 , acc: 0.483, loss: 0.976\n",
      "epoch: 3500 , acc: 0.470, loss: 0.925\n",
      "epoch: 3600 , acc: 0.463, loss: 0.968\n",
      "epoch: 3700 , acc: 0.523, loss: 0.894\n",
      "epoch: 3800 , acc: 0.530, loss: 0.881\n",
      "epoch: 3900 , acc: 0.567, loss: 0.924\n",
      "epoch: 4000 , acc: 0.547, loss: 0.875\n",
      "epoch: 4100 , acc: 0.530, loss: 0.852\n",
      "epoch: 4200 , acc: 0.580, loss: 0.805\n",
      "epoch: 4300 , acc: 0.597, loss: 0.791\n",
      "epoch: 4400 , acc: 0.623, loss: 0.790\n",
      "epoch: 4500 , acc: 0.587, loss: 0.783\n",
      "epoch: 4600 , acc: 0.613, loss: 0.748\n",
      "epoch: 4700 , acc: 0.603, loss: 0.844\n",
      "epoch: 4800 , acc: 0.620, loss: 0.733\n",
      "epoch: 4900 , acc: 0.683, loss: 0.647\n",
      "epoch: 5000 , acc: 0.623, loss: 0.702\n",
      "epoch: 5100 , acc: 0.640, loss: 0.721\n",
      "epoch: 5200 , acc: 0.680, loss: 0.661\n",
      "epoch: 5300 , acc: 0.713, loss: 0.616\n",
      "epoch: 5400 , acc: 0.600, loss: 0.825\n",
      "epoch: 5500 , acc: 0.683, loss: 0.638\n",
      "epoch: 5600 , acc: 0.687, loss: 0.669\n",
      "epoch: 5700 , acc: 0.683, loss: 0.646\n",
      "epoch: 5800 , acc: 0.693, loss: 0.636\n",
      "epoch: 5900 , acc: 0.683, loss: 0.640\n",
      "epoch: 6000 , acc: 0.697, loss: 0.626\n",
      "epoch: 6100 , acc: 0.683, loss: 0.630\n",
      "epoch: 6200 , acc: 0.693, loss: 0.621\n",
      "epoch: 6300 , acc: 0.700, loss: 0.611\n",
      "epoch: 6400 , acc: 0.703, loss: 0.608\n",
      "epoch: 6500 , acc: 0.707, loss: 0.618\n",
      "epoch: 6600 , acc: 0.700, loss: 0.620\n",
      "epoch: 6700 , acc: 0.733, loss: 0.592\n",
      "epoch: 6800 , acc: 0.710, loss: 0.602\n",
      "epoch: 6900 , acc: 0.717, loss: 0.623\n",
      "epoch: 7000 , acc: 0.700, loss: 0.673\n",
      "epoch: 7100 , acc: 0.720, loss: 0.596\n",
      "epoch: 7200 , acc: 0.740, loss: 0.576\n",
      "epoch: 7300 , acc: 0.770, loss: 0.568\n",
      "epoch: 7400 , acc: 0.733, loss: 0.566\n",
      "epoch: 7500 , acc: 0.710, loss: 0.616\n",
      "epoch: 7600 , acc: 0.740, loss: 0.536\n",
      "epoch: 7700 , acc: 0.743, loss: 0.520\n",
      "epoch: 7800 , acc: 0.777, loss: 0.526\n",
      "epoch: 7900 , acc: 0.787, loss: 0.514\n",
      "epoch: 8000 , acc: 0.783, loss: 0.508\n",
      "epoch: 8100 , acc: 0.770, loss: 0.508\n",
      "epoch: 8200 , acc: 0.633, loss: 0.865\n",
      "epoch: 8300 , acc: 0.770, loss: 0.490\n",
      "epoch: 8400 , acc: 0.797, loss: 0.483\n",
      "epoch: 8500 , acc: 0.807, loss: 0.474\n",
      "epoch: 8600 , acc: 0.777, loss: 0.473\n",
      "epoch: 8700 , acc: 0.787, loss: 0.469\n",
      "epoch: 8800 , acc: 0.790, loss: 0.459\n",
      "epoch: 8900 , acc: 0.790, loss: 0.457\n",
      "epoch: 9000 , acc: 0.810, loss: 0.456\n",
      "epoch: 9100 , acc: 0.817, loss: 0.447\n",
      "epoch: 9200 , acc: 0.817, loss: 0.446\n",
      "epoch: 9300 , acc: 0.817, loss: 0.451\n",
      "epoch: 9400 , acc: 0.787, loss: 0.455\n",
      "epoch: 9500 , acc: 0.783, loss: 0.455\n",
      "epoch: 9600 , acc: 0.790, loss: 0.457\n",
      "epoch: 9700 , acc: 0.813, loss: 0.436\n",
      "epoch: 9800 , acc: 0.823, loss: 0.434\n",
      "epoch: 9900 , acc: 0.830, loss: 0.431\n",
      "epoch: 10000 , acc: 0.830, loss: 0.430\n"
     ]
    }
   ],
   "source": [
    "#Multiple passes\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = NN.Layer_Dense(2,64)\n",
    "activation1 = NN.Activation_ReLU()\n",
    "dense2 = NN.Layer_Dense(64,3)\n",
    "loss_activation = NN.Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    #Print every 100th epoch\n",
    "    if not epoch % 100 :\n",
    "        print (f'epoch: {epoch} , ' +\n",
    "               f'acc: {accuracy :.3f}, ' +\n",
    "               f'loss: {loss :.3f}' )\n",
    "        \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.290, loss: 1.099lr: 1.0\n",
      "epoch: 100 , acc: 0.440, loss: 1.090lr: 0.5025125628140703\n",
      "epoch: 200 , acc: 0.463, loss: 1.073lr: 0.33444816053511706\n",
      "epoch: 300 , acc: 0.440, loss: 1.069lr: 0.2506265664160401\n",
      "epoch: 400 , acc: 0.440, loss: 1.067lr: 0.2004008016032064\n",
      "epoch: 500 , acc: 0.440, loss: 1.066lr: 0.1669449081803005\n",
      "epoch: 600 , acc: 0.443, loss: 1.066lr: 0.14306151645207438\n",
      "epoch: 700 , acc: 0.443, loss: 1.065lr: 0.1251564455569462\n",
      "epoch: 800 , acc: 0.440, loss: 1.065lr: 0.11123470522803114\n",
      "epoch: 900 , acc: 0.443, loss: 1.065lr: 0.10010010010010009\n",
      "epoch: 1000 , acc: 0.440, loss: 1.065lr: 0.09099181073703366\n",
      "epoch: 1100 , acc: 0.437, loss: 1.065lr: 0.08340283569641367\n",
      "epoch: 1200 , acc: 0.437, loss: 1.065lr: 0.07698229407236336\n",
      "epoch: 1300 , acc: 0.440, loss: 1.065lr: 0.07147962830593281\n",
      "epoch: 1400 , acc: 0.443, loss: 1.065lr: 0.066711140760507\n",
      "epoch: 1500 , acc: 0.443, loss: 1.064lr: 0.06253908692933083\n",
      "epoch: 1600 , acc: 0.443, loss: 1.064lr: 0.05885815185403177\n",
      "epoch: 1700 , acc: 0.443, loss: 1.064lr: 0.055586436909394105\n",
      "epoch: 1800 , acc: 0.443, loss: 1.064lr: 0.052659294365455495\n",
      "epoch: 1900 , acc: 0.443, loss: 1.064lr: 0.05002501250625312\n",
      "epoch: 2000 , acc: 0.443, loss: 1.064lr: 0.047641734159123386\n",
      "epoch: 2100 , acc: 0.440, loss: 1.064lr: 0.04547521600727603\n",
      "epoch: 2200 , acc: 0.440, loss: 1.064lr: 0.04349717268377555\n",
      "epoch: 2300 , acc: 0.440, loss: 1.064lr: 0.04168403501458941\n",
      "epoch: 2400 , acc: 0.440, loss: 1.064lr: 0.04001600640256102\n",
      "epoch: 2500 , acc: 0.440, loss: 1.064lr: 0.03847633705271258\n",
      "epoch: 2600 , acc: 0.440, loss: 1.064lr: 0.03705075954057058\n",
      "epoch: 2700 , acc: 0.443, loss: 1.064lr: 0.03572704537334762\n",
      "epoch: 2800 , acc: 0.443, loss: 1.064lr: 0.03449465332873405\n",
      "epoch: 2900 , acc: 0.440, loss: 1.064lr: 0.03334444814938312\n",
      "epoch: 3000 , acc: 0.440, loss: 1.064lr: 0.03226847370119393\n",
      "epoch: 3100 , acc: 0.440, loss: 1.064lr: 0.03125976867771178\n",
      "epoch: 3200 , acc: 0.440, loss: 1.064lr: 0.03031221582297666\n",
      "epoch: 3300 , acc: 0.440, loss: 1.064lr: 0.02942041776993233\n",
      "epoch: 3400 , acc: 0.440, loss: 1.064lr: 0.028579594169762787\n",
      "epoch: 3500 , acc: 0.440, loss: 1.064lr: 0.027785495971103084\n",
      "epoch: 3600 , acc: 0.440, loss: 1.064lr: 0.02703433360367667\n",
      "epoch: 3700 , acc: 0.440, loss: 1.064lr: 0.026322716504343247\n",
      "epoch: 3800 , acc: 0.440, loss: 1.064lr: 0.025647601949217745\n",
      "epoch: 3900 , acc: 0.440, loss: 1.064lr: 0.02500625156289072\n",
      "epoch: 4000 , acc: 0.440, loss: 1.064lr: 0.02439619419370578\n",
      "epoch: 4100 , acc: 0.440, loss: 1.064lr: 0.023815194093831864\n",
      "epoch: 4200 , acc: 0.440, loss: 1.064lr: 0.02326122354035822\n",
      "epoch: 4300 , acc: 0.440, loss: 1.064lr: 0.022732439190725165\n",
      "epoch: 4400 , acc: 0.440, loss: 1.064lr: 0.02222716159146477\n",
      "epoch: 4500 , acc: 0.443, loss: 1.064lr: 0.021743857360295715\n",
      "epoch: 4600 , acc: 0.443, loss: 1.064lr: 0.021281123643328365\n",
      "epoch: 4700 , acc: 0.443, loss: 1.064lr: 0.02083767451552407\n",
      "epoch: 4800 , acc: 0.443, loss: 1.064lr: 0.020412329046744233\n",
      "epoch: 4900 , acc: 0.443, loss: 1.064lr: 0.020004000800160033\n",
      "epoch: 5000 , acc: 0.443, loss: 1.064lr: 0.019611688566385566\n",
      "epoch: 5100 , acc: 0.443, loss: 1.064lr: 0.019234468166955183\n",
      "epoch: 5200 , acc: 0.443, loss: 1.063lr: 0.018871485185884128\n",
      "epoch: 5300 , acc: 0.443, loss: 1.063lr: 0.018521948508983144\n",
      "epoch: 5400 , acc: 0.440, loss: 1.063lr: 0.01818512456810329\n",
      "epoch: 5500 , acc: 0.440, loss: 1.063lr: 0.01786033220217896\n",
      "epoch: 5600 , acc: 0.440, loss: 1.063lr: 0.01754693805930865\n",
      "epoch: 5700 , acc: 0.440, loss: 1.063lr: 0.01724435247456458\n",
      "epoch: 5800 , acc: 0.440, loss: 1.063lr: 0.016952025767079167\n",
      "epoch: 5900 , acc: 0.440, loss: 1.063lr: 0.01666944490748458\n",
      "epoch: 6000 , acc: 0.440, loss: 1.063lr: 0.016396130513198885\n",
      "epoch: 6100 , acc: 0.440, loss: 1.063lr: 0.016131634134537828\n",
      "epoch: 6200 , acc: 0.440, loss: 1.063lr: 0.015875535799333228\n",
      "epoch: 6300 , acc: 0.440, loss: 1.063lr: 0.01562744178777934\n",
      "epoch: 6400 , acc: 0.440, loss: 1.063lr: 0.015386982612709646\n",
      "epoch: 6500 , acc: 0.440, loss: 1.063lr: 0.015153811183512654\n",
      "epoch: 6600 , acc: 0.440, loss: 1.063lr: 0.014927601134497688\n",
      "epoch: 6700 , acc: 0.440, loss: 1.063lr: 0.014708045300779527\n",
      "epoch: 6800 , acc: 0.440, loss: 1.063lr: 0.014494854326714018\n",
      "epoch: 6900 , acc: 0.440, loss: 1.063lr: 0.014287755393627663\n",
      "epoch: 7000 , acc: 0.440, loss: 1.063lr: 0.014086491055078181\n",
      "epoch: 7100 , acc: 0.440, loss: 1.063lr: 0.013890818169190166\n",
      "epoch: 7200 , acc: 0.440, loss: 1.063lr: 0.013700506918755994\n",
      "epoch: 7300 , acc: 0.440, loss: 1.063lr: 0.013515339910798757\n",
      "epoch: 7400 , acc: 0.440, loss: 1.063lr: 0.013335111348179758\n",
      "epoch: 7500 , acc: 0.440, loss: 1.063lr: 0.013159626266614028\n",
      "epoch: 7600 , acc: 0.440, loss: 1.063lr: 0.012988699831146902\n",
      "epoch: 7700 , acc: 0.440, loss: 1.063lr: 0.012822156686754713\n",
      "epoch: 7800 , acc: 0.437, loss: 1.063lr: 0.0126598303582732\n",
      "epoch: 7900 , acc: 0.437, loss: 1.063lr: 0.012501562695336917\n",
      "epoch: 8000 , acc: 0.440, loss: 1.063lr: 0.012347203358439314\n",
      "epoch: 8100 , acc: 0.437, loss: 1.063lr: 0.012196609342602758\n",
      "epoch: 8200 , acc: 0.437, loss: 1.063lr: 0.012049644535486204\n",
      "epoch: 8300 , acc: 0.437, loss: 1.063lr: 0.011906179307060364\n",
      "epoch: 8400 , acc: 0.437, loss: 1.063lr: 0.011766090128250382\n",
      "epoch: 8500 , acc: 0.437, loss: 1.063lr: 0.01162925921618793\n",
      "epoch: 8600 , acc: 0.437, loss: 1.063lr: 0.011495574203931488\n",
      "epoch: 8700 , acc: 0.437, loss: 1.063lr: 0.011364927832708264\n",
      "epoch: 8800 , acc: 0.437, loss: 1.063lr: 0.011237217664906169\n",
      "epoch: 8900 , acc: 0.437, loss: 1.063lr: 0.011112345816201801\n",
      "epoch: 9000 , acc: 0.437, loss: 1.063lr: 0.010990218705352238\n",
      "epoch: 9100 , acc: 0.437, loss: 1.063lr: 0.010870746820306556\n",
      "epoch: 9200 , acc: 0.440, loss: 1.063lr: 0.010753844499408539\n",
      "epoch: 9300 , acc: 0.440, loss: 1.063lr: 0.010639429726566656\n",
      "epoch: 9400 , acc: 0.440, loss: 1.063lr: 0.010527423939362039\n",
      "epoch: 9500 , acc: 0.440, loss: 1.063lr: 0.010417751849150954\n",
      "epoch: 9600 , acc: 0.440, loss: 1.063lr: 0.010310341272296112\n",
      "epoch: 9700 , acc: 0.440, loss: 1.063lr: 0.010205122971731808\n",
      "epoch: 9800 , acc: 0.440, loss: 1.063lr: 0.010102030508132133\n",
      "epoch: 9900 , acc: 0.440, loss: 1.063lr: 0.01000100010001\n",
      "epoch: 10000 , acc: 0.440, loss: 1.063lr: 0.009901970492127933\n"
     ]
    }
   ],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD :\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__ ( self , learning_rate = 1. , decay = 0. ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params ( self ):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params ( self , layer ):\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases\n",
    "    \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params ( self ):\n",
    "        self.iterations += 1\n",
    "        \n",
    "\n",
    "#Multiple passes\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = NN.Layer_Dense(2,64)\n",
    "activation1 = NN.Activation_ReLU()\n",
    "dense2 = NN.Layer_Dense(64,3)\n",
    "loss_activation = NN.Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD(decay=1e-2)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    #Print every 100th epoch\n",
    "    if not epoch % 100 :\n",
    "        print (f'epoch: {epoch} , ' +\n",
    "               f'acc: {accuracy :.3f}, ' +\n",
    "               f'loss: {loss :.3f}' +\n",
    "               f'lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.377, loss: 1.099lr: 1.0\n",
      "epoch: 100 , acc: 0.427, loss: 1.059lr: 0.9099181073703367\n",
      "epoch: 200 , acc: 0.473, loss: 0.951lr: 0.8340283569641367\n",
      "epoch: 300 , acc: 0.687, loss: 0.750lr: 0.7698229407236336\n",
      "epoch: 400 , acc: 0.780, loss: 0.556lr: 0.7147962830593281\n",
      "epoch: 500 , acc: 0.863, loss: 0.372lr: 0.66711140760507\n",
      "epoch: 600 , acc: 0.887, loss: 0.307lr: 0.6253908692933083\n",
      "epoch: 700 , acc: 0.917, loss: 0.247lr: 0.5885815185403178\n",
      "epoch: 800 , acc: 0.923, loss: 0.217lr: 0.5558643690939411\n",
      "epoch: 900 , acc: 0.923, loss: 0.187lr: 0.526592943654555\n",
      "epoch: 1000 , acc: 0.930, loss: 0.175lr: 0.5002501250625312\n",
      "epoch: 1100 , acc: 0.940, loss: 0.158lr: 0.4764173415912339\n",
      "epoch: 1200 , acc: 0.950, loss: 0.147lr: 0.45475216007276037\n",
      "epoch: 1300 , acc: 0.950, loss: 0.141lr: 0.43497172683775553\n",
      "epoch: 1400 , acc: 0.957, loss: 0.134lr: 0.4168403501458941\n",
      "epoch: 1500 , acc: 0.957, loss: 0.129lr: 0.4001600640256102\n",
      "epoch: 1600 , acc: 0.963, loss: 0.125lr: 0.3847633705271258\n",
      "epoch: 1700 , acc: 0.963, loss: 0.121lr: 0.3705075954057058\n",
      "epoch: 1800 , acc: 0.967, loss: 0.117lr: 0.35727045373347627\n",
      "epoch: 1900 , acc: 0.967, loss: 0.115lr: 0.3449465332873405\n",
      "epoch: 2000 , acc: 0.967, loss: 0.112lr: 0.33344448149383127\n",
      "epoch: 2100 , acc: 0.967, loss: 0.110lr: 0.32268473701193934\n",
      "epoch: 2200 , acc: 0.967, loss: 0.108lr: 0.31259768677711786\n",
      "epoch: 2300 , acc: 0.967, loss: 0.106lr: 0.3031221582297666\n",
      "epoch: 2400 , acc: 0.967, loss: 0.104lr: 0.29420417769932333\n",
      "epoch: 2500 , acc: 0.967, loss: 0.103lr: 0.2857959416976279\n",
      "epoch: 2600 , acc: 0.967, loss: 0.101lr: 0.2778549597110308\n",
      "epoch: 2700 , acc: 0.967, loss: 0.100lr: 0.2703433360367667\n",
      "epoch: 2800 , acc: 0.970, loss: 0.099lr: 0.26322716504343247\n",
      "epoch: 2900 , acc: 0.970, loss: 0.098lr: 0.25647601949217746\n",
      "epoch: 3000 , acc: 0.970, loss: 0.097lr: 0.25006251562890724\n",
      "epoch: 3100 , acc: 0.970, loss: 0.096lr: 0.2439619419370578\n",
      "epoch: 3200 , acc: 0.970, loss: 0.095lr: 0.23815194093831865\n",
      "epoch: 3300 , acc: 0.970, loss: 0.094lr: 0.23261223540358225\n",
      "epoch: 3400 , acc: 0.973, loss: 0.093lr: 0.22732439190725165\n",
      "epoch: 3500 , acc: 0.973, loss: 0.092lr: 0.22227161591464767\n",
      "epoch: 3600 , acc: 0.973, loss: 0.091lr: 0.21743857360295715\n",
      "epoch: 3700 , acc: 0.973, loss: 0.091lr: 0.21281123643328367\n",
      "epoch: 3800 , acc: 0.973, loss: 0.090lr: 0.20837674515524068\n",
      "epoch: 3900 , acc: 0.973, loss: 0.089lr: 0.20412329046744235\n",
      "epoch: 4000 , acc: 0.973, loss: 0.089lr: 0.2000400080016003\n",
      "epoch: 4100 , acc: 0.973, loss: 0.088lr: 0.19611688566385566\n",
      "epoch: 4200 , acc: 0.973, loss: 0.088lr: 0.19234468166955185\n",
      "epoch: 4300 , acc: 0.973, loss: 0.087lr: 0.18871485185884126\n",
      "epoch: 4400 , acc: 0.973, loss: 0.087lr: 0.18521948508983144\n",
      "epoch: 4500 , acc: 0.973, loss: 0.086lr: 0.18185124568103292\n",
      "epoch: 4600 , acc: 0.973, loss: 0.086lr: 0.1786033220217896\n",
      "epoch: 4700 , acc: 0.973, loss: 0.085lr: 0.1754693805930865\n",
      "epoch: 4800 , acc: 0.973, loss: 0.085lr: 0.17244352474564578\n",
      "epoch: 4900 , acc: 0.973, loss: 0.085lr: 0.16952025767079165\n",
      "epoch: 5000 , acc: 0.973, loss: 0.084lr: 0.16669444907484582\n",
      "epoch: 5100 , acc: 0.973, loss: 0.084lr: 0.16396130513198884\n",
      "epoch: 5200 , acc: 0.973, loss: 0.084lr: 0.16131634134537828\n",
      "epoch: 5300 , acc: 0.977, loss: 0.083lr: 0.15875535799333226\n",
      "epoch: 5400 , acc: 0.977, loss: 0.083lr: 0.1562744178777934\n",
      "epoch: 5500 , acc: 0.977, loss: 0.083lr: 0.15386982612709646\n",
      "epoch: 5600 , acc: 0.977, loss: 0.082lr: 0.15153811183512653\n",
      "epoch: 5700 , acc: 0.973, loss: 0.082lr: 0.14927601134497687\n",
      "epoch: 5800 , acc: 0.977, loss: 0.082lr: 0.14708045300779526\n",
      "epoch: 5900 , acc: 0.977, loss: 0.081lr: 0.14494854326714016\n",
      "epoch: 6000 , acc: 0.977, loss: 0.081lr: 0.1428775539362766\n",
      "epoch: 6100 , acc: 0.977, loss: 0.081lr: 0.1408649105507818\n",
      "epoch: 6200 , acc: 0.977, loss: 0.081lr: 0.13890818169190167\n",
      "epoch: 6300 , acc: 0.977, loss: 0.080lr: 0.13700506918755992\n",
      "epoch: 6400 , acc: 0.977, loss: 0.080lr: 0.13515339910798757\n",
      "epoch: 6500 , acc: 0.977, loss: 0.080lr: 0.13335111348179757\n",
      "epoch: 6600 , acc: 0.977, loss: 0.080lr: 0.13159626266614027\n",
      "epoch: 6700 , acc: 0.977, loss: 0.080lr: 0.12988699831146902\n",
      "epoch: 6800 , acc: 0.977, loss: 0.079lr: 0.12822156686754713\n",
      "epoch: 6900 , acc: 0.977, loss: 0.079lr: 0.126598303582732\n",
      "epoch: 7000 , acc: 0.977, loss: 0.079lr: 0.12501562695336915\n",
      "epoch: 7100 , acc: 0.977, loss: 0.079lr: 0.12347203358439313\n",
      "epoch: 7200 , acc: 0.973, loss: 0.079lr: 0.12196609342602757\n",
      "epoch: 7300 , acc: 0.977, loss: 0.078lr: 0.12049644535486204\n",
      "epoch: 7400 , acc: 0.977, loss: 0.078lr: 0.11906179307060363\n",
      "epoch: 7500 , acc: 0.977, loss: 0.078lr: 0.11766090128250381\n",
      "epoch: 7600 , acc: 0.977, loss: 0.078lr: 0.11629259216187929\n",
      "epoch: 7700 , acc: 0.977, loss: 0.078lr: 0.11495574203931487\n",
      "epoch: 7800 , acc: 0.977, loss: 0.078lr: 0.11364927832708263\n",
      "epoch: 7900 , acc: 0.977, loss: 0.077lr: 0.11237217664906168\n",
      "epoch: 8000 , acc: 0.977, loss: 0.077lr: 0.11112345816201799\n",
      "epoch: 8100 , acc: 0.977, loss: 0.077lr: 0.10990218705352237\n",
      "epoch: 8200 , acc: 0.973, loss: 0.077lr: 0.10870746820306555\n",
      "epoch: 8300 , acc: 0.977, loss: 0.077lr: 0.1075384449940854\n",
      "epoch: 8400 , acc: 0.973, loss: 0.077lr: 0.10639429726566654\n",
      "epoch: 8500 , acc: 0.977, loss: 0.076lr: 0.10527423939362038\n",
      "epoch: 8600 , acc: 0.973, loss: 0.076lr: 0.10417751849150952\n",
      "epoch: 8700 , acc: 0.977, loss: 0.076lr: 0.10310341272296113\n",
      "epoch: 8800 , acc: 0.977, loss: 0.076lr: 0.1020512297173181\n",
      "epoch: 8900 , acc: 0.977, loss: 0.076lr: 0.10102030508132134\n",
      "epoch: 9000 , acc: 0.973, loss: 0.076lr: 0.1000100010001\n",
      "epoch: 9100 , acc: 0.977, loss: 0.076lr: 0.09901970492127933\n",
      "epoch: 9200 , acc: 0.977, loss: 0.076lr: 0.09804882831650162\n",
      "epoch: 9300 , acc: 0.973, loss: 0.075lr: 0.09709680551509856\n",
      "epoch: 9400 , acc: 0.973, loss: 0.075lr: 0.09616309260505818\n",
      "epoch: 9500 , acc: 0.973, loss: 0.075lr: 0.09524716639679968\n",
      "epoch: 9600 , acc: 0.977, loss: 0.075lr: 0.09434852344560807\n",
      "epoch: 9700 , acc: 0.973, loss: 0.075lr: 0.09346667912889055\n",
      "epoch: 9800 , acc: 0.977, loss: 0.075lr: 0.09260116677470137\n",
      "epoch: 9900 , acc: 0.973, loss: 0.075lr: 0.09175153683824203\n",
      "epoch: 10000 , acc: 0.973, loss: 0.075lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "class Optimizer_SGD :\n",
    "    \n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__ ( self , learning_rate = 1. , decay = 0. , momentum = 0. ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params ( self ):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params ( self , layer ):\n",
    "        \n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them filled with zeros\n",
    "            if not hasattr (layer, 'weight_momentums' ):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else :\n",
    "            weight_updates = - self.current_learning_rate * layer.dweights\n",
    "            bias_updates = - self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params ( self ):\n",
    "        self.iterations += 1\n",
    "        \n",
    "        \n",
    "#Multiple passes\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = NN.Layer_Dense(2,64)\n",
    "activation1 = NN.Activation_ReLU()\n",
    "dense2 = NN.Layer_Dense(64,3)\n",
    "loss_activation = NN.Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    #Print every 100th epoch\n",
    "    if not epoch % 100 :\n",
    "        print (f'epoch: {epoch} , ' +\n",
    "               f'acc: {accuracy :.3f}, ' +\n",
    "               f'loss: {loss :.3f}' +\n",
    "               f'lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive gradient: uses a per-parameter learning rate rather than a globally-shared rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.383, loss: 1.099lr: 1.0\n",
      "epoch: 100 , acc: 0.467, loss: 1.012lr: 0.9901970492127933\n",
      "epoch: 200 , acc: 0.517, loss: 0.962lr: 0.9804882831650161\n",
      "epoch: 300 , acc: 0.473, loss: 0.959lr: 0.9709680551509855\n",
      "epoch: 400 , acc: 0.580, loss: 0.863lr: 0.9616309260505818\n",
      "epoch: 500 , acc: 0.600, loss: 0.811lr: 0.9524716639679969\n",
      "epoch: 600 , acc: 0.610, loss: 0.780lr: 0.9434852344560807\n",
      "epoch: 700 , acc: 0.647, loss: 0.734lr: 0.9346667912889054\n",
      "epoch: 800 , acc: 0.673, loss: 0.708lr: 0.9260116677470135\n",
      "epoch: 900 , acc: 0.687, loss: 0.679lr: 0.9175153683824203\n",
      "epoch: 1000 , acc: 0.680, loss: 0.649lr: 0.9091735612328392\n",
      "epoch: 1100 , acc: 0.690, loss: 0.631lr: 0.9009820704567978\n",
      "epoch: 1200 , acc: 0.697, loss: 0.606lr: 0.892936869363336\n",
      "epoch: 1300 , acc: 0.697, loss: 0.603lr: 0.8850340738118416\n",
      "epoch: 1400 , acc: 0.730, loss: 0.570lr: 0.8772699359592947\n",
      "epoch: 1500 , acc: 0.733, loss: 0.559lr: 0.8696408383337683\n",
      "epoch: 1600 , acc: 0.750, loss: 0.539lr: 0.8621432882145013\n",
      "epoch: 1700 , acc: 0.760, loss: 0.525lr: 0.8547739123001966\n",
      "epoch: 1800 , acc: 0.767, loss: 0.516lr: 0.8475294516484448\n",
      "epoch: 1900 , acc: 0.770, loss: 0.506lr: 0.8404067568703253\n",
      "epoch: 2000 , acc: 0.773, loss: 0.497lr: 0.8334027835652972\n",
      "epoch: 2100 , acc: 0.787, loss: 0.490lr: 0.8265145879824779\n",
      "epoch: 2200 , acc: 0.800, loss: 0.478lr: 0.8197393228953193\n",
      "epoch: 2300 , acc: 0.803, loss: 0.470lr: 0.8130742336775347\n",
      "epoch: 2400 , acc: 0.810, loss: 0.461lr: 0.8065166545689169\n",
      "epoch: 2500 , acc: 0.817, loss: 0.452lr: 0.8000640051204096\n",
      "epoch: 2600 , acc: 0.813, loss: 0.446lr: 0.7937137868084768\n",
      "epoch: 2700 , acc: 0.823, loss: 0.438lr: 0.7874635798094338\n",
      "epoch: 2800 , acc: 0.820, loss: 0.432lr: 0.7813110399249941\n",
      "epoch: 2900 , acc: 0.813, loss: 0.434lr: 0.7752538956508256\n",
      "epoch: 3000 , acc: 0.817, loss: 0.428lr: 0.7692899453804138\n",
      "epoch: 3100 , acc: 0.827, loss: 0.421lr: 0.7634170547370028\n",
      "epoch: 3200 , acc: 0.830, loss: 0.415lr: 0.7576331540268202\n",
      "epoch: 3300 , acc: 0.830, loss: 0.408lr: 0.7519362358072035\n",
      "epoch: 3400 , acc: 0.837, loss: 0.404lr: 0.7463243525636241\n",
      "epoch: 3500 , acc: 0.837, loss: 0.399lr: 0.7407956144899621\n",
      "epoch: 3600 , acc: 0.840, loss: 0.397lr: 0.735348187366718\n",
      "epoch: 3700 , acc: 0.840, loss: 0.395lr: 0.7299802905321557\n",
      "epoch: 3800 , acc: 0.840, loss: 0.390lr: 0.7246901949416624\n",
      "epoch: 3900 , acc: 0.843, loss: 0.385lr: 0.7194762213108857\n",
      "epoch: 4000 , acc: 0.847, loss: 0.384lr: 0.7143367383384527\n",
      "epoch: 4100 , acc: 0.847, loss: 0.380lr: 0.7092701610043266\n",
      "epoch: 4200 , acc: 0.850, loss: 0.375lr: 0.7042749489400663\n",
      "epoch: 4300 , acc: 0.850, loss: 0.373lr: 0.6993496048674733\n",
      "epoch: 4400 , acc: 0.853, loss: 0.370lr: 0.6944926731022988\n",
      "epoch: 4500 , acc: 0.850, loss: 0.367lr: 0.6897027381198704\n",
      "epoch: 4600 , acc: 0.853, loss: 0.365lr: 0.6849784231796698\n",
      "epoch: 4700 , acc: 0.853, loss: 0.362lr: 0.6803183890060548\n",
      "epoch: 4800 , acc: 0.853, loss: 0.361lr: 0.6757213325224677\n",
      "epoch: 4900 , acc: 0.853, loss: 0.358lr: 0.6711859856366199\n",
      "epoch: 5000 , acc: 0.853, loss: 0.356lr: 0.6667111140742716\n",
      "epoch: 5100 , acc: 0.857, loss: 0.354lr: 0.6622955162593549\n",
      "epoch: 5200 , acc: 0.853, loss: 0.352lr: 0.6579380222383051\n",
      "epoch: 5300 , acc: 0.853, loss: 0.350lr: 0.6536374926465782\n",
      "epoch: 5400 , acc: 0.850, loss: 0.348lr: 0.649392817715436\n",
      "epoch: 5500 , acc: 0.857, loss: 0.346lr: 0.6452029163171817\n",
      "epoch: 5600 , acc: 0.850, loss: 0.345lr: 0.6410667350471184\n",
      "epoch: 5700 , acc: 0.853, loss: 0.343lr: 0.6369832473405949\n",
      "epoch: 5800 , acc: 0.847, loss: 0.342lr: 0.6329514526235838\n",
      "epoch: 5900 , acc: 0.853, loss: 0.340lr: 0.6289703754953141\n",
      "epoch: 6000 , acc: 0.857, loss: 0.339lr: 0.6250390649415589\n",
      "epoch: 6100 , acc: 0.857, loss: 0.338lr: 0.6211565935772407\n",
      "epoch: 6200 , acc: 0.857, loss: 0.337lr: 0.6173220569170937\n",
      "epoch: 6300 , acc: 0.857, loss: 0.335lr: 0.6135345726731701\n",
      "epoch: 6400 , acc: 0.853, loss: 0.334lr: 0.6097932800780536\n",
      "epoch: 6500 , acc: 0.853, loss: 0.333lr: 0.6060973392326807\n",
      "epoch: 6600 , acc: 0.853, loss: 0.332lr: 0.6024459304777396\n",
      "epoch: 6700 , acc: 0.853, loss: 0.331lr: 0.5988382537876519\n",
      "epoch: 6800 , acc: 0.860, loss: 0.330lr: 0.5952735281862016\n",
      "epoch: 6900 , acc: 0.860, loss: 0.329lr: 0.5917509911829102\n",
      "epoch: 7000 , acc: 0.860, loss: 0.328lr: 0.5882698982293076\n",
      "epoch: 7100 , acc: 0.860, loss: 0.327lr: 0.5848295221942803\n",
      "epoch: 7200 , acc: 0.860, loss: 0.326lr: 0.5814291528577243\n",
      "epoch: 7300 , acc: 0.860, loss: 0.325lr: 0.5780680964217585\n",
      "epoch: 7400 , acc: 0.860, loss: 0.325lr: 0.5747456750387954\n",
      "epoch: 7500 , acc: 0.860, loss: 0.324lr: 0.5714612263557918\n",
      "epoch: 7600 , acc: 0.860, loss: 0.323lr: 0.5682141030740383\n",
      "epoch: 7700 , acc: 0.860, loss: 0.323lr: 0.5650036725238714\n",
      "epoch: 7800 , acc: 0.860, loss: 0.322lr: 0.5618293162537221\n",
      "epoch: 7900 , acc: 0.860, loss: 0.321lr: 0.5586904296329404\n",
      "epoch: 8000 , acc: 0.860, loss: 0.321lr: 0.5555864214678593\n",
      "epoch: 8100 , acc: 0.860, loss: 0.320lr: 0.5525167136305873\n",
      "epoch: 8200 , acc: 0.860, loss: 0.319lr: 0.5494807407000385\n",
      "epoch: 8300 , acc: 0.860, loss: 0.319lr: 0.5464779496147331\n",
      "epoch: 8400 , acc: 0.860, loss: 0.318lr: 0.5435077993369205\n",
      "epoch: 8500 , acc: 0.860, loss: 0.317lr: 0.5405697605275961\n",
      "epoch: 8600 , acc: 0.857, loss: 0.317lr: 0.5376633152320017\n",
      "epoch: 8700 , acc: 0.857, loss: 0.316lr: 0.5347879565752179\n",
      "epoch: 8800 , acc: 0.857, loss: 0.316lr: 0.5319431884674717\n",
      "epoch: 8900 , acc: 0.860, loss: 0.315lr: 0.5291285253188\n",
      "epoch: 9000 , acc: 0.857, loss: 0.314lr: 0.5263434917627243\n",
      "epoch: 9100 , acc: 0.857, loss: 0.313lr: 0.5235876223886068\n",
      "epoch: 9200 , acc: 0.857, loss: 0.313lr: 0.5208604614823689\n",
      "epoch: 9300 , acc: 0.863, loss: 0.312lr: 0.5181615627752734\n",
      "epoch: 9400 , acc: 0.867, loss: 0.311lr: 0.5154904892004742\n",
      "epoch: 9500 , acc: 0.860, loss: 0.311lr: 0.5128468126570593\n",
      "epoch: 9600 , acc: 0.860, loss: 0.310lr: 0.5102301137813153\n",
      "epoch: 9700 , acc: 0.867, loss: 0.310lr: 0.5076399817249606\n",
      "epoch: 9800 , acc: 0.867, loss: 0.309lr: 0.5050760139400979\n",
      "epoch: 9900 , acc: 0.863, loss: 0.308lr: 0.5025378159706518\n",
      "epoch: 10000 , acc: 0.863, loss: 0.308lr: 0.5000250012500626\n"
     ]
    }
   ],
   "source": [
    "class Optimizer_Adagrad :\n",
    "    \n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__ ( self , learning_rate = 1. , decay = 0. , epsilon = 1e-7 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params ( self ):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
    " \n",
    "    # Update parameters\n",
    "    def update_params ( self , layer ):\n",
    "        \n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr (layer, 'weight_cache' ):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += - self.current_learning_rate * \\\n",
    "                layer.dweights / \\\n",
    "                (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * \\\n",
    "            layer.dbiases / \\\n",
    "                (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "                \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params ( self ):\n",
    "        self.iterations += 1\n",
    "        \n",
    "        \n",
    " \n",
    "#Multiple passes\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = NN.Layer_Dense(2,64)\n",
    "activation1 = NN.Activation_ReLU()\n",
    "dense2 = NN.Layer_Dense(64,3)\n",
    "loss_activation = NN.Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_Adagrad( decay = 1e-4 )\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    #Print every 100th epoch\n",
    "    if not epoch % 100 :\n",
    "        print (f'epoch: {epoch} , ' +\n",
    "               f'acc: {accuracy :.3f}, ' +\n",
    "               f'loss: {loss :.3f}' +\n",
    "               f'lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Propagation: another adaptive learning rate\n",
    "\n",
    "Adds momentum for a smoother learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.323, loss: 1.099lr: 0.02\n",
      "epoch: 100 , acc: 0.447, loss: 1.045lr: 0.01998021958261321\n",
      "epoch: 200 , acc: 0.473, loss: 1.005lr: 0.019960279044701046\n",
      "epoch: 300 , acc: 0.450, loss: 0.983lr: 0.019940378268975763\n",
      "epoch: 400 , acc: 0.503, loss: 0.958lr: 0.01992051713662487\n",
      "epoch: 500 , acc: 0.483, loss: 0.970lr: 0.01990069552930875\n",
      "epoch: 600 , acc: 0.527, loss: 0.906lr: 0.019880913329158343\n",
      "epoch: 700 , acc: 0.530, loss: 0.878lr: 0.019861170418772778\n",
      "epoch: 800 , acc: 0.547, loss: 0.891lr: 0.019841466681217078\n",
      "epoch: 900 , acc: 0.557, loss: 0.840lr: 0.01982180200001982\n",
      "epoch: 1000 , acc: 0.613, loss: 0.815lr: 0.019802176259170884\n",
      "epoch: 1100 , acc: 0.617, loss: 0.814lr: 0.01978258934311912\n",
      "epoch: 1200 , acc: 0.607, loss: 0.791lr: 0.01976304113677013\n",
      "epoch: 1300 , acc: 0.593, loss: 0.780lr: 0.019743531525483964\n",
      "epoch: 1400 , acc: 0.610, loss: 0.760lr: 0.01972406039507293\n",
      "epoch: 1500 , acc: 0.597, loss: 0.750lr: 0.019704627631799327\n",
      "epoch: 1600 , acc: 0.667, loss: 0.721lr: 0.019685233122373254\n",
      "epoch: 1700 , acc: 0.637, loss: 0.717lr: 0.019665876753950384\n",
      "epoch: 1800 , acc: 0.637, loss: 0.708lr: 0.01964655841412981\n",
      "epoch: 1900 , acc: 0.720, loss: 0.680lr: 0.019627277990951823\n",
      "epoch: 2000 , acc: 0.687, loss: 0.680lr: 0.019608035372895814\n",
      "epoch: 2100 , acc: 0.707, loss: 0.661lr: 0.01958883044887805\n",
      "epoch: 2200 , acc: 0.640, loss: 0.688lr: 0.019569663108249594\n",
      "epoch: 2300 , acc: 0.703, loss: 0.634lr: 0.01955053324079414\n",
      "epoch: 2400 , acc: 0.733, loss: 0.608lr: 0.019531440736725945\n",
      "epoch: 2500 , acc: 0.697, loss: 0.612lr: 0.019512385486687673\n",
      "epoch: 2600 , acc: 0.740, loss: 0.594lr: 0.019493367381748363\n",
      "epoch: 2700 , acc: 0.767, loss: 0.565lr: 0.019474386313401298\n",
      "epoch: 2800 , acc: 0.747, loss: 0.578lr: 0.019455442173562\n",
      "epoch: 2900 , acc: 0.763, loss: 0.568lr: 0.019436534854566128\n",
      "epoch: 3000 , acc: 0.730, loss: 0.579lr: 0.01941766424916747\n",
      "epoch: 3100 , acc: 0.747, loss: 0.568lr: 0.019398830250535893\n",
      "epoch: 3200 , acc: 0.753, loss: 0.549lr: 0.019380032752255354\n",
      "epoch: 3300 , acc: 0.767, loss: 0.528lr: 0.01936127164832186\n",
      "epoch: 3400 , acc: 0.757, loss: 0.540lr: 0.01934254683314152\n",
      "epoch: 3500 , acc: 0.770, loss: 0.526lr: 0.019323858201528515\n",
      "epoch: 3600 , acc: 0.760, loss: 0.542lr: 0.019305205648703173\n",
      "epoch: 3700 , acc: 0.763, loss: 0.544lr: 0.01928658907028997\n",
      "epoch: 3800 , acc: 0.777, loss: 0.519lr: 0.01926800836231563\n",
      "epoch: 3900 , acc: 0.763, loss: 0.521lr: 0.019249463421207133\n",
      "epoch: 4000 , acc: 0.760, loss: 0.517lr: 0.019230954143789846\n",
      "epoch: 4100 , acc: 0.787, loss: 0.495lr: 0.019212480427285565\n",
      "epoch: 4200 , acc: 0.777, loss: 0.501lr: 0.019194042169310647\n",
      "epoch: 4300 , acc: 0.750, loss: 0.538lr: 0.019175639267874092\n",
      "epoch: 4400 , acc: 0.753, loss: 0.509lr: 0.019157271621375684\n",
      "epoch: 4500 , acc: 0.773, loss: 0.533lr: 0.0191389391286041\n",
      "epoch: 4600 , acc: 0.777, loss: 0.494lr: 0.019120641688735073\n",
      "epoch: 4700 , acc: 0.787, loss: 0.475lr: 0.019102379201329525\n",
      "epoch: 4800 , acc: 0.797, loss: 0.474lr: 0.01908415156633174\n",
      "epoch: 4900 , acc: 0.820, loss: 0.435lr: 0.01906595868406753\n",
      "epoch: 5000 , acc: 0.793, loss: 0.459lr: 0.01904780045524243\n",
      "epoch: 5100 , acc: 0.793, loss: 0.453lr: 0.019029676780939874\n",
      "epoch: 5200 , acc: 0.810, loss: 0.436lr: 0.019011587562619416\n",
      "epoch: 5300 , acc: 0.817, loss: 0.447lr: 0.01899353270211493\n",
      "epoch: 5400 , acc: 0.820, loss: 0.441lr: 0.018975512101632844\n",
      "epoch: 5500 , acc: 0.790, loss: 0.414lr: 0.018957525663750367\n",
      "epoch: 5600 , acc: 0.810, loss: 0.438lr: 0.018939573291413745\n",
      "epoch: 5700 , acc: 0.797, loss: 0.442lr: 0.018921654887936498\n",
      "epoch: 5800 , acc: 0.803, loss: 0.444lr: 0.018903770356997706\n",
      "epoch: 5900 , acc: 0.810, loss: 0.431lr: 0.018885919602640248\n",
      "epoch: 6000 , acc: 0.810, loss: 0.424lr: 0.018868102529269144\n",
      "epoch: 6100 , acc: 0.820, loss: 0.435lr: 0.018850319041649778\n",
      "epoch: 6200 , acc: 0.823, loss: 0.423lr: 0.018832569044906263\n",
      "epoch: 6300 , acc: 0.823, loss: 0.423lr: 0.018814852444519702\n",
      "epoch: 6400 , acc: 0.690, loss: 0.730lr: 0.018797169146326564\n",
      "epoch: 6500 , acc: 0.803, loss: 0.420lr: 0.01877951905651696\n",
      "epoch: 6600 , acc: 0.797, loss: 0.417lr: 0.018761902081633034\n",
      "epoch: 6700 , acc: 0.793, loss: 0.421lr: 0.018744318128567278\n",
      "epoch: 6800 , acc: 0.830, loss: 0.417lr: 0.018726767104560903\n",
      "epoch: 6900 , acc: 0.827, loss: 0.409lr: 0.018709248917202218\n",
      "epoch: 7000 , acc: 0.827, loss: 0.406lr: 0.018691763474424996\n",
      "epoch: 7100 , acc: 0.820, loss: 0.408lr: 0.018674310684506857\n",
      "epoch: 7200 , acc: 0.800, loss: 0.393lr: 0.01865689045606769\n",
      "epoch: 7300 , acc: 0.803, loss: 0.398lr: 0.01863950269806802\n",
      "epoch: 7400 , acc: 0.803, loss: 0.398lr: 0.018622147319807447\n",
      "epoch: 7500 , acc: 0.803, loss: 0.400lr: 0.018604824230923075\n",
      "epoch: 7600 , acc: 0.830, loss: 0.376lr: 0.01858753334138793\n",
      "epoch: 7700 , acc: 0.833, loss: 0.390lr: 0.018570274561509396\n",
      "epoch: 7800 , acc: 0.830, loss: 0.391lr: 0.018553047801927663\n",
      "epoch: 7900 , acc: 0.830, loss: 0.391lr: 0.018535852973614212\n",
      "epoch: 8000 , acc: 0.803, loss: 0.390lr: 0.01851868998787026\n",
      "epoch: 8100 , acc: 0.827, loss: 0.383lr: 0.018501558756325222\n",
      "epoch: 8200 , acc: 0.830, loss: 0.381lr: 0.01848445919093522\n",
      "epoch: 8300 , acc: 0.830, loss: 0.383lr: 0.018467391203981567\n",
      "epoch: 8400 , acc: 0.730, loss: 0.642lr: 0.018450354708069265\n",
      "epoch: 8500 , acc: 0.840, loss: 0.379lr: 0.018433349616125496\n",
      "epoch: 8600 , acc: 0.847, loss: 0.377lr: 0.018416375841398172\n",
      "epoch: 8700 , acc: 0.843, loss: 0.375lr: 0.01839943329745444\n",
      "epoch: 8800 , acc: 0.847, loss: 0.373lr: 0.01838252189817921\n",
      "epoch: 8900 , acc: 0.840, loss: 0.343lr: 0.018365641557773718\n",
      "epoch: 9000 , acc: 0.843, loss: 0.355lr: 0.018348792190754044\n",
      "epoch: 9100 , acc: 0.840, loss: 0.361lr: 0.0183319737119497\n",
      "epoch: 9200 , acc: 0.837, loss: 0.365lr: 0.018315186036502167\n",
      "epoch: 9300 , acc: 0.837, loss: 0.357lr: 0.018298429079863496\n",
      "epoch: 9400 , acc: 0.573, loss: 1.219lr: 0.018281702757794862\n",
      "epoch: 9500 , acc: 0.857, loss: 0.335lr: 0.018265006986365174\n",
      "epoch: 9600 , acc: 0.863, loss: 0.341lr: 0.018248341681949654\n",
      "epoch: 9700 , acc: 0.860, loss: 0.347lr: 0.018231706761228456\n",
      "epoch: 9800 , acc: 0.860, loss: 0.343lr: 0.018215102141185255\n",
      "epoch: 9900 , acc: 0.860, loss: 0.341lr: 0.018198527739105907\n",
      "epoch: 10000 , acc: 0.840, loss: 0.366lr: 0.018181983472577025\n"
     ]
    }
   ],
   "source": [
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop :\n",
    "    \n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__ ( self , learning_rate = 0.001 , decay = 0. , epsilon = 1e-7 , rho = 0.9 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params ( self ):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params ( self , layer ):\n",
    "        \n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr (layer, 'weight_cache' ):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + ( 1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + ( 1 - self.rho) * layer.dbiases ** 2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / \\\n",
    "            (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * \\\n",
    "            layer.dbiases / \\\n",
    "                (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "                \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params ( self ):\n",
    "        self.iterations += 1\n",
    "        \n",
    "\n",
    "#Multiple passes\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = NN.Layer_Dense(2,64)\n",
    "activation1 = NN.Activation_ReLU()\n",
    "dense2 = NN.Layer_Dense(64,3)\n",
    "loss_activation = NN.Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_RMSprop(learning_rate=0.02, decay = 1e-5, rho=0.999 )\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    #Print every 100th epoch\n",
    "    if not epoch % 100 :\n",
    "        print (f'epoch: {epoch} , ' +\n",
    "               f'acc: {accuracy :.3f}, ' +\n",
    "               f'loss: {loss :.3f}' +\n",
    "               f'lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Momentum: Most widly used optimiser, built atop RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.377, loss: 1.099lr: 0.05\n",
      "epoch: 100 , acc: 0.733, loss: 0.664lr: 0.04999752512250644\n",
      "epoch: 200 , acc: 0.787, loss: 0.506lr: 0.04999502549496326\n",
      "epoch: 300 , acc: 0.803, loss: 0.438lr: 0.049992526117345455\n",
      "epoch: 400 , acc: 0.810, loss: 0.409lr: 0.04999002698961558\n",
      "epoch: 500 , acc: 0.840, loss: 0.377lr: 0.049987528111736124\n",
      "epoch: 600 , acc: 0.863, loss: 0.356lr: 0.049985029483669646\n",
      "epoch: 700 , acc: 0.850, loss: 0.345lr: 0.049982531105378675\n",
      "epoch: 800 , acc: 0.897, loss: 0.292lr: 0.04998003297682575\n",
      "epoch: 900 , acc: 0.903, loss: 0.260lr: 0.049977535097973466\n",
      "epoch: 1000 , acc: 0.897, loss: 0.250lr: 0.049975037468784345\n",
      "epoch: 1100 , acc: 0.917, loss: 0.236lr: 0.049972540089220974\n",
      "epoch: 1200 , acc: 0.917, loss: 0.220lr: 0.04997004295924593\n",
      "epoch: 1300 , acc: 0.920, loss: 0.209lr: 0.04996754607882181\n",
      "epoch: 1400 , acc: 0.927, loss: 0.206lr: 0.049965049447911185\n",
      "epoch: 1500 , acc: 0.913, loss: 0.197lr: 0.04996255306647668\n",
      "epoch: 1600 , acc: 0.930, loss: 0.185lr: 0.049960056934480884\n",
      "epoch: 1700 , acc: 0.927, loss: 0.172lr: 0.04995756105188642\n",
      "epoch: 1800 , acc: 0.943, loss: 0.162lr: 0.049955065418655915\n",
      "epoch: 1900 , acc: 0.940, loss: 0.155lr: 0.04995257003475201\n",
      "epoch: 2000 , acc: 0.947, loss: 0.147lr: 0.04995007490013731\n",
      "epoch: 2100 , acc: 0.950, loss: 0.142lr: 0.0499475800147745\n",
      "epoch: 2200 , acc: 0.937, loss: 0.151lr: 0.0499450853786262\n",
      "epoch: 2300 , acc: 0.953, loss: 0.133lr: 0.0499425909916551\n",
      "epoch: 2400 , acc: 0.853, loss: 0.474lr: 0.04994009685382384\n",
      "epoch: 2500 , acc: 0.950, loss: 0.130lr: 0.04993760296509512\n",
      "epoch: 2600 , acc: 0.953, loss: 0.127lr: 0.049935109325431604\n",
      "epoch: 2700 , acc: 0.960, loss: 0.125lr: 0.049932615934796004\n",
      "epoch: 2800 , acc: 0.960, loss: 0.123lr: 0.04993012279315098\n",
      "epoch: 2900 , acc: 0.960, loss: 0.121lr: 0.049927629900459285\n",
      "epoch: 3000 , acc: 0.953, loss: 0.119lr: 0.049925137256683606\n",
      "epoch: 3100 , acc: 0.963, loss: 0.117lr: 0.04992264486178666\n",
      "epoch: 3200 , acc: 0.963, loss: 0.116lr: 0.04992015271573119\n",
      "epoch: 3300 , acc: 0.943, loss: 0.130lr: 0.04991766081847992\n",
      "epoch: 3400 , acc: 0.923, loss: 0.164lr: 0.049915169169995596\n",
      "epoch: 3500 , acc: 0.963, loss: 0.110lr: 0.049912677770240964\n",
      "epoch: 3600 , acc: 0.963, loss: 0.109lr: 0.049910186619178794\n",
      "epoch: 3700 , acc: 0.963, loss: 0.108lr: 0.04990769571677183\n",
      "epoch: 3800 , acc: 0.930, loss: 0.127lr: 0.04990520506298287\n",
      "epoch: 3900 , acc: 0.967, loss: 0.106lr: 0.04990271465777467\n",
      "epoch: 4000 , acc: 0.967, loss: 0.104lr: 0.049900224501110035\n",
      "epoch: 4100 , acc: 0.967, loss: 0.103lr: 0.04989773459295174\n",
      "epoch: 4200 , acc: 0.967, loss: 0.102lr: 0.04989524493326262\n",
      "epoch: 4300 , acc: 0.963, loss: 0.101lr: 0.04989275552200545\n",
      "epoch: 4400 , acc: 0.960, loss: 0.102lr: 0.04989026635914307\n",
      "epoch: 4500 , acc: 0.967, loss: 0.100lr: 0.04988777744463829\n",
      "epoch: 4600 , acc: 0.970, loss: 0.098lr: 0.049885288778453954\n",
      "epoch: 4700 , acc: 0.970, loss: 0.097lr: 0.049882800360552884\n",
      "epoch: 4800 , acc: 0.957, loss: 0.112lr: 0.04988031219089794\n",
      "epoch: 4900 , acc: 0.970, loss: 0.095lr: 0.049877824269451976\n",
      "epoch: 5000 , acc: 0.970, loss: 0.094lr: 0.04987533659617785\n",
      "epoch: 5100 , acc: 0.970, loss: 0.093lr: 0.04987284917103844\n",
      "epoch: 5200 , acc: 0.970, loss: 0.093lr: 0.04987036199399661\n",
      "epoch: 5300 , acc: 0.970, loss: 0.092lr: 0.04986787506501525\n",
      "epoch: 5400 , acc: 0.970, loss: 0.091lr: 0.04986538838405724\n",
      "epoch: 5500 , acc: 0.970, loss: 0.090lr: 0.049862901951085496\n",
      "epoch: 5600 , acc: 0.970, loss: 0.090lr: 0.049860415766062906\n",
      "epoch: 5700 , acc: 0.970, loss: 0.089lr: 0.0498579298289524\n",
      "epoch: 5800 , acc: 0.943, loss: 0.132lr: 0.04985544413971689\n",
      "epoch: 5900 , acc: 0.970, loss: 0.089lr: 0.049852958698319315\n",
      "epoch: 6000 , acc: 0.970, loss: 0.088lr: 0.04985047350472258\n",
      "epoch: 6100 , acc: 0.970, loss: 0.087lr: 0.04984798855888967\n",
      "epoch: 6200 , acc: 0.970, loss: 0.087lr: 0.049845503860783506\n",
      "epoch: 6300 , acc: 0.970, loss: 0.086lr: 0.049843019410367055\n",
      "epoch: 6400 , acc: 0.970, loss: 0.086lr: 0.04984053520760327\n",
      "epoch: 6500 , acc: 0.970, loss: 0.085lr: 0.049838051252455155\n",
      "epoch: 6600 , acc: 0.970, loss: 0.085lr: 0.049835567544885655\n",
      "epoch: 6700 , acc: 0.970, loss: 0.084lr: 0.04983308408485778\n",
      "epoch: 6800 , acc: 0.970, loss: 0.083lr: 0.0498306008723345\n",
      "epoch: 6900 , acc: 0.970, loss: 0.083lr: 0.04982811790727884\n",
      "epoch: 7000 , acc: 0.963, loss: 0.091lr: 0.04982563518965381\n",
      "epoch: 7100 , acc: 0.967, loss: 0.084lr: 0.049823152719422406\n",
      "epoch: 7200 , acc: 0.967, loss: 0.083lr: 0.049820670496547675\n",
      "epoch: 7300 , acc: 0.967, loss: 0.083lr: 0.04981818852099264\n",
      "epoch: 7400 , acc: 0.967, loss: 0.083lr: 0.049815706792720335\n",
      "epoch: 7500 , acc: 0.967, loss: 0.082lr: 0.0498132253116938\n",
      "epoch: 7600 , acc: 0.967, loss: 0.082lr: 0.04981074407787611\n",
      "epoch: 7700 , acc: 0.970, loss: 0.081lr: 0.049808263091230306\n",
      "epoch: 7800 , acc: 0.970, loss: 0.081lr: 0.04980578235171948\n",
      "epoch: 7900 , acc: 0.970, loss: 0.081lr: 0.04980330185930667\n",
      "epoch: 8000 , acc: 0.970, loss: 0.080lr: 0.04980082161395499\n",
      "epoch: 8100 , acc: 0.970, loss: 0.080lr: 0.04979834161562752\n",
      "epoch: 8200 , acc: 0.970, loss: 0.079lr: 0.04979586186428736\n",
      "epoch: 8300 , acc: 0.973, loss: 0.079lr: 0.04979338235989761\n",
      "epoch: 8400 , acc: 0.967, loss: 0.078lr: 0.04979090310242139\n",
      "epoch: 8500 , acc: 0.967, loss: 0.080lr: 0.049788424091821805\n",
      "epoch: 8600 , acc: 0.973, loss: 0.078lr: 0.049785945328062006\n",
      "epoch: 8700 , acc: 0.973, loss: 0.078lr: 0.0497834668111051\n",
      "epoch: 8800 , acc: 0.973, loss: 0.077lr: 0.049780988540914256\n",
      "epoch: 8900 , acc: 0.973, loss: 0.077lr: 0.0497785105174526\n",
      "epoch: 9000 , acc: 0.973, loss: 0.076lr: 0.04977603274068329\n",
      "epoch: 9100 , acc: 0.973, loss: 0.076lr: 0.04977355521056952\n",
      "epoch: 9200 , acc: 0.973, loss: 0.076lr: 0.049771077927074414\n",
      "epoch: 9300 , acc: 0.973, loss: 0.075lr: 0.0497686008901612\n",
      "epoch: 9400 , acc: 0.973, loss: 0.075lr: 0.04976612409979302\n",
      "epoch: 9500 , acc: 0.973, loss: 0.075lr: 0.0497636475559331\n",
      "epoch: 9600 , acc: 0.973, loss: 0.074lr: 0.049761171258544616\n",
      "epoch: 9700 , acc: 0.973, loss: 0.074lr: 0.0497586952075908\n",
      "epoch: 9800 , acc: 0.973, loss: 0.073lr: 0.04975621940303483\n",
      "epoch: 9900 , acc: 0.973, loss: 0.073lr: 0.049753743844839965\n",
      "epoch: 10000 , acc: 0.973, loss: 0.073lr: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "class Optimizer_Adam :\n",
    "    \n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__ ( self , learning_rate = 0.001 , decay = 0. , epsilon = 1e-7 , beta_1 = 0.9 , beta_2 = 0.999 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params ( self ):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params ( self , layer ):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr (layer, 'weight_cache' ):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "            layer.weight_momentums + \\\n",
    "                ( 1 - self.beta_1) * layer.dweights\n",
    "                \n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "            layer.bias_momentums + \\\n",
    "                ( 1 - self.beta_1) * layer.dbiases\n",
    "                \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
    "            \n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
    "            \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            ( 1 - self.beta_2) * layer.dweights ** 2\n",
    "            \n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            ( 1 - self.beta_2) * layer.dbiases ** 2\n",
    "            \n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
    "            \n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
    "            \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += - self.current_learning_rate * \\\n",
    "            weight_momentums_corrected / \\\n",
    "                (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "                \n",
    "        layer.biases += - self.current_learning_rate * \\\n",
    "            bias_momentums_corrected / \\\n",
    "                (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "                \n",
    "                \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params ( self ):\n",
    "        self.iterations += 1\n",
    "        \n",
    "        \n",
    "\n",
    "#Multiple passes\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = NN.Layer_Dense(2,64)\n",
    "activation1 = NN.Activation_ReLU()\n",
    "dense2 = NN.Layer_Dense(64,3)\n",
    "loss_activation = NN.Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay = 5e-7)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    #Print every 100th epoch\n",
    "    if not epoch % 100 :\n",
    "        print (f'epoch: {epoch} , ' +\n",
    "               f'acc: {accuracy :.3f}, ' +\n",
    "               f'loss: {loss :.3f}' +\n",
    "               f'lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
