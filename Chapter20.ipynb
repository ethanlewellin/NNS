{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data, spiral_data\n",
    "import random\n",
    "import requests\n",
    "from NNS import NeuralNetwork as NN #import neural net code from github to reduce copy/pasting\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to model class\n",
    "Layer_Input = NN.Layer_Input()\n",
    "Activation_Softmax = NN.Activation_Softmax()\n",
    "Loss_CategoricalCrossentropy = NN.Loss_CategoricalCrossentropy()\n",
    "Activation_Softmax_Loss_CategoricalCrossentropy = NN.Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Model class\n",
    "class Model :\n",
    "    \n",
    "    def __init__ (self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    # Add objects to the model\n",
    "    def add (self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set (self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    # Finalize the model\n",
    "    def finalize (self):\n",
    "        \n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "            \n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0 :\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "                \n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1 :\n",
    "                self.layers[i].prev = self.layers[i - 1] \n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "                \n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else :\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "                # Update loss object with trainable layers\n",
    "                self.loss.remember_trainable_layers(\n",
    "                    self.trainable_layers\n",
    "                    )\n",
    "                \n",
    "                \n",
    "            # If output activation is Softmax and\n",
    "            # loss function is Categorical Cross-Entropy\n",
    "            # create an object of combined activation\n",
    "            # and loss function containing\n",
    "            # faster gradient calculation\n",
    "            if isinstance(self.layers[ - 1 ], Activation_Softmax) and \\\n",
    "                isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "                # Create an object of combined activation\n",
    "                # and loss functions\n",
    "                self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs = 1, batch_size=None, print_every = 1, validation_data = None):\n",
    "        \n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "        \n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None :\n",
    "            validation_steps = 1\n",
    "            \n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "            # Calculate number of steps\n",
    "            if batch_size is not None:\n",
    "                train_steps = len (X) // batch_size\n",
    "                \n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data, but not a full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if train_steps * batch_size < len (X):\n",
    "                    train_steps += 1\n",
    "                    \n",
    "                if validation_data is not None :\n",
    "                    validation_steps = len (X_val) // batch_size\n",
    "                    # Dividing rounds down. If there are some remaining\n",
    "                    # data, but nor full batch, this won't include it\n",
    "                    # Add `1` to include this not full batch\n",
    "                    if validation_steps * batch_size < len (X_val):\n",
    "                        validation_steps += 1\n",
    "        \n",
    "        # Main training loop\n",
    "        for epoch in range (1, epochs+1):\n",
    "\n",
    "            print (f'epoch: {epoch}')\n",
    "            \n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None :\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                    \n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size:(step + 1 ) * batch_size]\n",
    "                    batch_y = y[step * batch_size:(step + 1 ) * batch_size]\n",
    "                    \n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training = True)\n",
    "            \n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization = True)\n",
    "                loss = data_loss + regularization_loss\n",
    "            \n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "            \n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "            \n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "            \n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print ( f'step: {step} , ' +\n",
    "                            f'acc: {accuracy :.3f}, ' +\n",
    "                            f'loss: {loss :.3f} (' +\n",
    "                            f'data_loss: {data_loss :.3f} , ' +\n",
    "                            f'reg_loss: {regularization_loss :.3f}), ' +\n",
    "                            f'lr: {self.optimizer.current_learning_rate}')\n",
    "                    \n",
    "            # Get and print epoch loss and accuracy\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(\n",
    "                include_regularization = True)\n",
    "            \n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            \n",
    "            print ( f'training, ' +\n",
    "                    f'acc: {epoch_accuracy :.3f}, ' +\n",
    "                    f'loss: {epoch_loss :.3f} (' +\n",
    "                    f'data_loss: {epoch_data_loss :.3f}, ' +\n",
    "                    f'reg_loss: {epoch_regularization_loss :.3f} ), ' +\n",
    "                    f'lr: {self.optimizer.current_learning_rate}' )\n",
    "                \n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "                \n",
    "                self.evaluate(*validation_data, batch_size=batch_size)\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward ( self , X , training ):\n",
    "        \n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "    \n",
    "    # Performs backward pass\n",
    "    def backward ( self , output , y ):\n",
    "        \n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None :\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            \n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "            \n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "                \n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed (self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "            \n",
    "    def evaluate ( self , X_val , y_val , * , batch_size = None ):\n",
    "        \n",
    "        # Default value if batch size is not being set\n",
    "        validation_steps = 1\n",
    "        \n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            \n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            \n",
    "            # Add `1` to include this not full batch\n",
    "            if validation_steps * batch_size < len (X_val):\n",
    "                validation_steps += 1\n",
    "                \n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            # Iterate over steps\n",
    "            for step in range(validation_steps):\n",
    "                \n",
    "                # If batch size is not set, train using one step and full dataset\n",
    "                if batch_size is None:\n",
    "                    batch_X = X_val\n",
    "                    batch_y = y_val\n",
    "                    \n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X_val[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y_val[step*batch_size:(step+1)*batch_size]\n",
    "                 \n",
    "            # Perform the forward pass   \n",
    "            output = self.forward(batch_X, training=False)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            self.loss.calculate(output, batch_y)\n",
    "            \n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "            \n",
    "        # Get and print validation loss and accuracy\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "        \n",
    "        # Print a summary\n",
    "        print(f'validation, ' +\n",
    "              f'acc: {validation_accuracy:.3f},' +\n",
    "              f'loss: {validation_loss:.3f}')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_data_mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X, y, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data_mnist\u001b[49m( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfashion_mnist_images\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Shuffle the training dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m keys \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray( \u001b[38;5;28mrange\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[ \u001b[38;5;241m0\u001b[39m ]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_data_mnist' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'\n",
    "\n",
    "download_mnist_dataset(URL, FILE, FOLDER)\n",
    "X, y, X_test, y_test = create_data_mnist( 'fashion_mnist_images' )\n",
    "# Shuffle the training dataset\n",
    "keys = np.array( range (X.shape[ 0 ]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[ 0 ], - 1 ).astype(np.float32) -\n",
    "127.5 ) / 127.5\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[ 1 ], 128 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 128 , 128 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 128 , 10 ))\n",
    "model.add(Activation_Softmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
