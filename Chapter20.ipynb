{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data, spiral_data\n",
    "import random\n",
    "import requests\n",
    "from NNS import NeuralNetwork as NN #import neural net code from github to reduce copy/pasting\n",
    "from NNS import load_MNIST_Data\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to model class\n",
    "Layer_Input = NN.Layer_Input\n",
    "Activation_Softmax = NN.Activation_Softmax\n",
    "Loss_CategoricalCrossentropy = NN.Loss_CategoricalCrossentropy\n",
    "Activation_Softmax_Loss_CategoricalCrossentropy = NN.Activation_Softmax_Loss_CategoricalCrossentropy\n",
    "\n",
    "# Model class\n",
    "class Model :\n",
    "    \n",
    "    def __init__ (self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    # Add objects to the model\n",
    "    def add (self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set (self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    # Finalize the model\n",
    "    def finalize (self):\n",
    "        \n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "            \n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0 :\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "                \n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1 :\n",
    "                self.layers[i].prev = self.layers[i - 1] \n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "                \n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else :\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "                # Update loss object with trainable layers\n",
    "                self.loss.remember_trainable_layers(\n",
    "                    self.trainable_layers\n",
    "                    )\n",
    "                \n",
    "                \n",
    "            # If output activation is Softmax and\n",
    "            # loss function is Categorical Cross-Entropy\n",
    "            # create an object of combined activation\n",
    "            # and loss function containing\n",
    "            # faster gradient calculation\n",
    "            if isinstance(self.layers[ - 1 ], Activation_Softmax) and \\\n",
    "                isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "                # Create an object of combined activation\n",
    "                # and loss functions\n",
    "                self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs = 1, batch_size=None, print_every = 1, validation_data = None):\n",
    "        \n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "        \n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None :\n",
    "            validation_steps = 1\n",
    "            \n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "            # Calculate number of steps\n",
    "            if batch_size is not None:\n",
    "                train_steps = len (X) // batch_size\n",
    "                \n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data, but not a full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if train_steps * batch_size < len (X):\n",
    "                    train_steps += 1\n",
    "                    \n",
    "                if validation_data is not None :\n",
    "                    validation_steps = len (X_val) // batch_size\n",
    "                    # Dividing rounds down. If there are some remaining\n",
    "                    # data, but nor full batch, this won't include it\n",
    "                    # Add `1` to include this not full batch\n",
    "                    if validation_steps * batch_size < len (X_val):\n",
    "                        validation_steps += 1\n",
    "        \n",
    "        # Main training loop\n",
    "        for epoch in range (1, epochs+1):\n",
    "\n",
    "            print (f'epoch: {epoch}')\n",
    "            \n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None :\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                    \n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size:(step + 1 ) * batch_size]\n",
    "                    batch_y = y[step * batch_size:(step + 1 ) * batch_size]\n",
    "                    \n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training = True)\n",
    "            \n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization = True)\n",
    "                loss = data_loss + regularization_loss\n",
    "            \n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "            \n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "            \n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "            \n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print ( f'step: {step} , ' +\n",
    "                            f'acc: {accuracy :.3f}, ' +\n",
    "                            f'loss: {loss :.3f} (' +\n",
    "                            f'data_loss: {data_loss :.3f} , ' +\n",
    "                            f'reg_loss: {regularization_loss :.3f}), ' +\n",
    "                            f'lr: {self.optimizer.current_learning_rate}')\n",
    "                    \n",
    "            # Get and print epoch loss and accuracy\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(\n",
    "                include_regularization = True)\n",
    "            \n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            \n",
    "            print ( f'training, ' +\n",
    "                    f'acc: {epoch_accuracy :.3f}, ' +\n",
    "                    f'loss: {epoch_loss :.3f} (' +\n",
    "                    f'data_loss: {epoch_data_loss :.3f}, ' +\n",
    "                    f'reg_loss: {epoch_regularization_loss :.3f} ), ' +\n",
    "                    f'lr: {self.optimizer.current_learning_rate}' )\n",
    "                \n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "                \n",
    "                self.evaluate(*validation_data, batch_size=batch_size)\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward ( self , X , training ):\n",
    "        \n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "    \n",
    "    # Performs backward pass\n",
    "    def backward ( self , output , y ):\n",
    "        \n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None :\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            \n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "            \n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "                \n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed (self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "            \n",
    "    def evaluate ( self , X_val , y_val , * , batch_size = None ):\n",
    "        \n",
    "        # Default value if batch size is not being set\n",
    "        validation_steps = 1\n",
    "        \n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            \n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            \n",
    "            # Add `1` to include this not full batch\n",
    "            if validation_steps * batch_size < len (X_val):\n",
    "                validation_steps += 1\n",
    "                \n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            # Iterate over steps\n",
    "            for step in range(validation_steps):\n",
    "                \n",
    "                # If batch size is not set, train using one step and full dataset\n",
    "                if batch_size is None:\n",
    "                    batch_X = X_val\n",
    "                    batch_y = y_val\n",
    "                    \n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X_val[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y_val[step*batch_size:(step+1)*batch_size]\n",
    "                 \n",
    "            # Perform the forward pass   \n",
    "            output = self.forward(batch_X, training=False)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            self.loss.calculate(output, batch_y)\n",
    "            \n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "            \n",
    "        # Get and print validation loss and accuracy\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "        \n",
    "        # Print a summary\n",
    "        print(f'validation, ' +\n",
    "              f'acc: {validation_accuracy:.3f},' +\n",
    "              f'loss: {validation_loss:.3f}')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping images...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'\n",
    "\n",
    "#load_MNIST_Data.download_mnist_dataset(URL = URL, FILE = FILE, FOLDER = FOLDER)\n",
    "X, y, X_test, y_test = load_MNIST_Data.create_data_mnist( 'fashion_mnist_images' )\n",
    "\n",
    "# Shuffle the training dataset\n",
    "keys = np.array( range (X.shape[ 0 ]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0 , acc: 0.047, loss: 2.303 (data_loss: 2.303 , reg_loss: 0.000), lr: 0.001\n",
      "step: 100 , acc: 0.680, loss: 0.741 (data_loss: 0.741 , reg_loss: 0.000), lr: 0.0009090909090909091\n",
      "step: 200 , acc: 0.773, loss: 0.531 (data_loss: 0.531 , reg_loss: 0.000), lr: 0.0008333333333333334\n",
      "step: 300 , acc: 0.836, loss: 0.453 (data_loss: 0.453 , reg_loss: 0.000), lr: 0.0007692307692307692\n",
      "step: 400 , acc: 0.820, loss: 0.559 (data_loss: 0.559 , reg_loss: 0.000), lr: 0.0007142857142857143\n",
      "step: 468 , acc: 0.833, loss: 0.456 (data_loss: 0.456 , reg_loss: 0.000), lr: 0.000681198910081744\n",
      "training, acc: 0.762, loss: 0.642 (data_loss: 0.642, reg_loss: 0.000 ), lr: 0.000681198910081744\n",
      "validation, acc: 0.938,loss: 0.069\n",
      "epoch: 2\n",
      "step: 0 , acc: 0.781, loss: 0.665 (data_loss: 0.665 , reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "step: 100 , acc: 0.812, loss: 0.437 (data_loss: 0.437 , reg_loss: 0.000), lr: 0.0006373486297004461\n",
      "step: 200 , acc: 0.844, loss: 0.433 (data_loss: 0.433 , reg_loss: 0.000), lr: 0.0005991611743559018\n",
      "step: 300 , acc: 0.867, loss: 0.343 (data_loss: 0.343 , reg_loss: 0.000), lr: 0.0005652911249293386\n",
      "step: 400 , acc: 0.828, loss: 0.455 (data_loss: 0.455 , reg_loss: 0.000), lr: 0.0005350454788657037\n",
      "step: 468 , acc: 0.812, loss: 0.393 (data_loss: 0.393 , reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "training, acc: 0.847, loss: 0.420 (data_loss: 0.420, reg_loss: 0.000 ), lr: 0.0005162622612287042\n",
      "validation, acc: 1.000,loss: 0.056\n",
      "epoch: 3\n",
      "step: 0 , acc: 0.820, loss: 0.586 (data_loss: 0.586 , reg_loss: 0.000), lr: 0.0005159958720330237\n",
      "step: 100 , acc: 0.820, loss: 0.396 (data_loss: 0.396 , reg_loss: 0.000), lr: 0.0004906771344455348\n",
      "step: 200 , acc: 0.867, loss: 0.399 (data_loss: 0.399 , reg_loss: 0.000), lr: 0.0004677268475210477\n",
      "step: 300 , acc: 0.883, loss: 0.319 (data_loss: 0.319 , reg_loss: 0.000), lr: 0.00044682752457551384\n",
      "step: 400 , acc: 0.859, loss: 0.412 (data_loss: 0.412 , reg_loss: 0.000), lr: 0.00042771599657827206\n",
      "step: 468 , acc: 0.823, loss: 0.375 (data_loss: 0.375 , reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "training, acc: 0.862, loss: 0.378 (data_loss: 0.378, reg_loss: 0.000 ), lr: 0.0004156275976724854\n",
      "validation, acc: 1.000,loss: 0.047\n",
      "epoch: 4\n",
      "step: 0 , acc: 0.844, loss: 0.548 (data_loss: 0.548 , reg_loss: 0.000), lr: 0.0004154549231408392\n",
      "step: 100 , acc: 0.844, loss: 0.361 (data_loss: 0.361 , reg_loss: 0.000), lr: 0.00039888312724371757\n",
      "step: 200 , acc: 0.883, loss: 0.382 (data_loss: 0.382 , reg_loss: 0.000), lr: 0.0003835826620636747\n",
      "step: 300 , acc: 0.898, loss: 0.309 (data_loss: 0.309 , reg_loss: 0.000), lr: 0.0003694126339120798\n",
      "step: 400 , acc: 0.867, loss: 0.381 (data_loss: 0.381 , reg_loss: 0.000), lr: 0.0003562522265764161\n",
      "step: 468 , acc: 0.833, loss: 0.358 (data_loss: 0.358 , reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "training, acc: 0.871, loss: 0.354 (data_loss: 0.354, reg_loss: 0.000 ), lr: 0.00034782608695652176\n",
      "validation, acc: 1.000,loss: 0.045\n",
      "epoch: 5\n",
      "step: 0 , acc: 0.852, loss: 0.505 (data_loss: 0.505 , reg_loss: 0.000), lr: 0.0003477051460361613\n",
      "step: 100 , acc: 0.844, loss: 0.338 (data_loss: 0.338 , reg_loss: 0.000), lr: 0.00033602150537634406\n",
      "step: 200 , acc: 0.875, loss: 0.374 (data_loss: 0.374 , reg_loss: 0.000), lr: 0.00032509752925877764\n",
      "step: 300 , acc: 0.906, loss: 0.298 (data_loss: 0.298 , reg_loss: 0.000), lr: 0.00031486146095717883\n",
      "step: 400 , acc: 0.875, loss: 0.361 (data_loss: 0.361 , reg_loss: 0.000), lr: 0.00030525030525030525\n",
      "step: 468 , acc: 0.844, loss: 0.342 (data_loss: 0.342 , reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "training, acc: 0.877, loss: 0.338 (data_loss: 0.338, reg_loss: 0.000 ), lr: 0.0002990430622009569\n",
      "validation, acc: 1.000,loss: 0.047\n",
      "epoch: 6\n",
      "step: 0 , acc: 0.859, loss: 0.470 (data_loss: 0.470 , reg_loss: 0.000), lr: 0.0002989536621823617\n",
      "step: 100 , acc: 0.859, loss: 0.323 (data_loss: 0.323 , reg_loss: 0.000), lr: 0.00029027576197387516\n",
      "step: 200 , acc: 0.875, loss: 0.368 (data_loss: 0.368 , reg_loss: 0.000), lr: 0.0002820874471086037\n",
      "step: 300 , acc: 0.922, loss: 0.289 (data_loss: 0.289 , reg_loss: 0.000), lr: 0.00027434842249657066\n",
      "step: 400 , acc: 0.891, loss: 0.346 (data_loss: 0.346 , reg_loss: 0.000), lr: 0.000267022696929239\n",
      "step: 468 , acc: 0.854, loss: 0.323 (data_loss: 0.323 , reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "training, acc: 0.882, loss: 0.325 (data_loss: 0.325, reg_loss: 0.000 ), lr: 0.00026226068712300026\n",
      "validation, acc: 1.000,loss: 0.045\n",
      "epoch: 7\n",
      "step: 0 , acc: 0.867, loss: 0.443 (data_loss: 0.443 , reg_loss: 0.000), lr: 0.00026219192448872575\n",
      "step: 100 , acc: 0.883, loss: 0.308 (data_loss: 0.308 , reg_loss: 0.000), lr: 0.00025549310168625444\n",
      "step: 200 , acc: 0.875, loss: 0.363 (data_loss: 0.363 , reg_loss: 0.000), lr: 0.00024912805181863477\n",
      "step: 300 , acc: 0.922, loss: 0.283 (data_loss: 0.283 , reg_loss: 0.000), lr: 0.0002430724355858046\n",
      "step: 400 , acc: 0.891, loss: 0.331 (data_loss: 0.331 , reg_loss: 0.000), lr: 0.00023730422401518745\n",
      "step: 468 , acc: 0.854, loss: 0.310 (data_loss: 0.310 , reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "training, acc: 0.886, loss: 0.314 (data_loss: 0.314, reg_loss: 0.000 ), lr: 0.00023353573096683791\n",
      "validation, acc: 1.000,loss: 0.045\n",
      "epoch: 8\n",
      "step: 0 , acc: 0.875, loss: 0.418 (data_loss: 0.418 , reg_loss: 0.000), lr: 0.00023348120476301658\n",
      "step: 100 , acc: 0.898, loss: 0.295 (data_loss: 0.295 , reg_loss: 0.000), lr: 0.00022815423226100847\n",
      "step: 200 , acc: 0.891, loss: 0.359 (data_loss: 0.359 , reg_loss: 0.000), lr: 0.0002230649118893598\n",
      "step: 300 , acc: 0.930, loss: 0.278 (data_loss: 0.278 , reg_loss: 0.000), lr: 0.00021819768710451667\n",
      "step: 400 , acc: 0.883, loss: 0.322 (data_loss: 0.322 , reg_loss: 0.000), lr: 0.00021353833013025838\n",
      "step: 468 , acc: 0.875, loss: 0.297 (data_loss: 0.297 , reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "training, acc: 0.889, loss: 0.305 (data_loss: 0.305, reg_loss: 0.000 ), lr: 0.00021048200378867611\n",
      "validation, acc: 1.000,loss: 0.046\n",
      "epoch: 9\n",
      "step: 0 , acc: 0.883, loss: 0.399 (data_loss: 0.399 , reg_loss: 0.000), lr: 0.0002104377104377104\n",
      "step: 100 , acc: 0.906, loss: 0.284 (data_loss: 0.284 , reg_loss: 0.000), lr: 0.0002061005770816158\n",
      "step: 200 , acc: 0.891, loss: 0.352 (data_loss: 0.352 , reg_loss: 0.000), lr: 0.00020193861066235866\n",
      "step: 300 , acc: 0.930, loss: 0.275 (data_loss: 0.275 , reg_loss: 0.000), lr: 0.0001979414093428345\n",
      "step: 400 , acc: 0.891, loss: 0.314 (data_loss: 0.314 , reg_loss: 0.000), lr: 0.0001940993788819876\n",
      "step: 468 , acc: 0.875, loss: 0.287 (data_loss: 0.287 , reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "training, acc: 0.891, loss: 0.298 (data_loss: 0.298, reg_loss: 0.000 ), lr: 0.00019157088122605365\n",
      "validation, acc: 1.000,loss: 0.049\n",
      "epoch: 10\n",
      "step: 0 , acc: 0.883, loss: 0.385 (data_loss: 0.385 , reg_loss: 0.000), lr: 0.0001915341888527102\n",
      "step: 100 , acc: 0.906, loss: 0.274 (data_loss: 0.274 , reg_loss: 0.000), lr: 0.00018793459875963167\n",
      "step: 200 , acc: 0.891, loss: 0.344 (data_loss: 0.344 , reg_loss: 0.000), lr: 0.00018446781036709093\n",
      "step: 300 , acc: 0.922, loss: 0.272 (data_loss: 0.272 , reg_loss: 0.000), lr: 0.00018112660749864155\n",
      "step: 400 , acc: 0.891, loss: 0.308 (data_loss: 0.308 , reg_loss: 0.000), lr: 0.00017790428749332856\n",
      "step: 468 , acc: 0.896, loss: 0.279 (data_loss: 0.279 , reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "training, acc: 0.894, loss: 0.291 (data_loss: 0.291, reg_loss: 0.000 ), lr: 0.00017577781683951485\n",
      "validation, acc: 0.938,loss: 0.053\n",
      "Evaluate testing data: \n",
      "validation, acc: 0.938,loss: 0.053\n",
      "Evaluate training data: \n",
      "validation, acc: 0.938,loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(NN.Layer_Dense(X.shape[ 1 ], 128 ))\n",
    "model.add(NN.Activation_ReLU())\n",
    "model.add(NN.Layer_Dense( 128 , 128 ))\n",
    "model.add(NN.Activation_ReLU())\n",
    "model.add(NN.Layer_Dense( 128 , 10 ))\n",
    "model.add(NN.Activation_Softmax())\n",
    "model.set(\n",
    "loss = NN.Loss_CategoricalCrossentropy(),\n",
    "    optimizer = NN.Optimizer_Adam( decay = 1e-3 ),\n",
    "    accuracy = NN.Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "# Train the model\n",
    "model.train(X, y, validation_data = (X_test, y_test),\n",
    "            epochs = 10 , batch_size = 128 , print_every = 100 )\n",
    "\n",
    "print(\"Evaluate testing data: \")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Evaluate training data: \")\n",
    "model.evaluate(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
