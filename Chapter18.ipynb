{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data, spiral_data\n",
    "import random\n",
    "import requests\n",
    "from NNS import NeuralNetwork as NN #import neural net code from github to reduce copy/pasting\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to Activation Functions\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Activation Functions\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "#Relu Activation\n",
    "## On/off linear function, easy to optimize\n",
    "## Most popular, the \"go-to\" function\n",
    "## Can cause dying neurons \n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward Pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "    # Backward Pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() # don't want to modify original values\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "    # calculate predictions\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "        \n",
    "#Softmax Activation \n",
    "## Typically used in the last hidden layer\n",
    "## Calculates the probabilty distribution over 'n' different events\n",
    "## Dependant probabilites, sum of proabilites  = 1\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward Pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inpus = inputs\n",
    "        \n",
    "        #Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims = True))\n",
    "        # Normalize them for each sample\n",
    "        probabilites = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilites\n",
    "    \n",
    "    # Backward Pass\n",
    "    def backward(self, dvalues):    \n",
    "        \n",
    "        # Create uninitialized array\n",
    "        self.dinputs=np.empty_like(dvalues)\n",
    "        \n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            #Flatten output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            #Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            #Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "            \n",
    "    # Calculate prediction\n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "            \n",
    "# Sigmoid Activation\n",
    "## Creates values between 0 and 1\n",
    "## Difficult optimization becuase it is not 0-centered  \n",
    "## Good for classification\n",
    "## Independant probabilities =, sum of probabilities not necessarily equal to 1\n",
    "class Activate_Sigmoid:\n",
    "    \n",
    "    # Add confidence value needed for model to consider a 'pass'\n",
    "    def __init__(self, confidence = 0.5):\n",
    "        self.confidence = confidence\n",
    "    \n",
    "    #Forward Pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input and calculate/save output of sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1/(1+np.exp(-inputs))\n",
    "            \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        \n",
    "    # Calculate predictions\n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > self.confidence) * 1\n",
    "        \n",
    "    \n",
    "# Linear activation function\n",
    "# Most basic regression activation function    \n",
    "class Activation_Linear:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # Backward Pass\n",
    "    def backward(self, dvalues):\n",
    "        # deriviative of linear function is 1\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "    # Calculate predictions\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to general loss function      \n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Loss Functions\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "#Common Loss Class\n",
    "class Loss:\n",
    "    \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate ( self, output, y, *, include_regularization = False):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # Calculate regulariation loss\n",
    "        # iterate all trainable layers\n",
    "        \n",
    "        for layer in self.trainable_layers:\n",
    "            \n",
    "            # L1 regularization - weights\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            \n",
    "            # L2 regularization - weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "                \n",
    "            # L1 regularization - biases\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "                \n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "            \n",
    "        return regularization_loss\n",
    "    \n",
    "    # Set/remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "    \n",
    "# Categorical Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    #Forward Pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "            \n",
    "        #Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "            \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not affect mean\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1- 1e-7)\n",
    "           \n",
    "        # Probabilities for target values\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "                \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis = 1\n",
    "            )\n",
    "                \n",
    "        #Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "        \n",
    "     # Backward pass\n",
    "    def backward ( self , dvalues , y_true ):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len (dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len (dvalues[ 0 ])\n",
    "        \n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len (y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "# Binary Cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    # Forward Pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7 , 1 - 1e-7 )\n",
    "        \n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        return sample_losses\n",
    "    \n",
    "    #Backward Pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Clip the data to prevent division by 0\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7 , 1 - 1e-7 )\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues - \n",
    "                        (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normailize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "   \n",
    "# Mean Squared Error (L2) Loss\n",
    "class Loss_MeanSquaredError(Loss): # L2 loss\n",
    "    \n",
    "    # Forward Pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of Samples\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        #Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # Normalize Gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "# Mean Absolute Error (L1) Loss        \n",
    "class Loss_MeanAbsoluteError(Loss): # L1 loss\n",
    "    \n",
    "    # Forward Pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis = -1)\n",
    "        \n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Calculate Gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize Gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Combined Activation and Loss Functions\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len (y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1 )\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[ range (samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Class and additional classes\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Model Class\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Model class\n",
    "class Model :\n",
    "    \n",
    "    def __init__ (self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    # Add objects to the model\n",
    "    def add (self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set (self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    # Finalize the model\n",
    "    def finalize (self):\n",
    "        \n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "            \n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0 :\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "                \n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1 :\n",
    "                self.layers[i].prev = self.layers[i - 1] \n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "                \n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else :\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "                # Update loss object with trainable layers\n",
    "                self.loss.remember_trainable_layers(\n",
    "                    self.trainable_layers\n",
    "                    )\n",
    "                \n",
    "                \n",
    "            # If output activation is Softmax and\n",
    "            # loss function is Categorical Cross-Entropy\n",
    "            # create an object of combined activation\n",
    "            # and loss function containing\n",
    "            # faster gradient calculation\n",
    "            if isinstance(self.layers[ - 1 ], Activation_Softmax) and \\\n",
    "                isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "                # Create an object of combined activation\n",
    "                # and loss functions\n",
    "                self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs = 1, print_every = 1, validation_data = None):\n",
    "        \n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        # Main training loop\n",
    "        for epoch in range (1, epochs+1):\n",
    "\n",
    "            # Perform the forward pass\n",
    "            output = self.forward(X, training = True)\n",
    "            \n",
    "            # Calculate loss\n",
    "            data_loss, regularization_loss = self.loss.calculate(output, y, include_regularization = True)\n",
    "            loss = data_loss + regularization_loss\n",
    "            \n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y)\n",
    "            \n",
    "            # Perform backward pass\n",
    "            self.backward(output, y)\n",
    "            \n",
    "            # Optimize (update parameters)\n",
    "            self.optimizer.pre_update_params()\n",
    "            for layer in self.trainable_layers:\n",
    "                self.optimizer.update_params(layer)\n",
    "            self.optimizer.post_update_params()\n",
    "            \n",
    "            # Print a summary\n",
    "            if not epoch % print_every:\n",
    "                print ( f'epoch: {epoch} , ' +\n",
    "                        f'acc: {accuracy :.3f}, ' +\n",
    "                        f'loss: {loss :.3f} (' +\n",
    "                        f'data_loss: {data_loss :.3f} , ' +\n",
    "                        f'reg_loss: {regularization_loss :.3f}), ' +\n",
    "                        f'lr: {self.optimizer.current_learning_rate}')\n",
    "                \n",
    "                # If there is the validation data\n",
    "                if validation_data is not None:\n",
    "                    # For better readability\n",
    "                    X_val, y_val = validation_data\n",
    "                    \n",
    "                    # Perform the forward pass\n",
    "                    output = self.forward(X_val, training=False)\n",
    "                    \n",
    "                    # Calculate the loss\n",
    "                    loss = self.loss.calculate(output, y_val)\n",
    "                    \n",
    "                    # Get predictions and calculate an accuracy\n",
    "                    predictions = self.output_layer_activation.predictions(output)\n",
    "                    accuracy = self.accuracy.calculate(predictions, y_val)\n",
    "\n",
    "                    # Print a summary\n",
    "                    print ( f'validation, ' +\n",
    "                            f'acc: {accuracy :.3f} , ' +\n",
    "                            f'loss: {loss :.3f}')\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward ( self , X , training ):\n",
    "        \n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "    \n",
    "    # Performs backward pass\n",
    "    def backward ( self , output , y ):\n",
    "        \n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None :\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            \n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "            \n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "                \n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed (self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "        \n",
    "# Input \"Layer\"\n",
    "class Layer_Input:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "        \n",
    "        \n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Accuracy Classes\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Common Accuracy Class\n",
    "class Accuracy:\n",
    "    \n",
    "    # Calculates an accuracy\n",
    "    # given prediction and ground truth values\n",
    "    def calculate(self, predictions, y):\n",
    "        \n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "        \n",
    "        # Calculate an accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        # Return accuracy\n",
    "        return accuracy\n",
    "    \n",
    "# Accuracy calculation for regression\n",
    "class Accuracy_Regression(Accuracy):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    # Calculates precision value\n",
    "    # based on passed in ground truth\n",
    "    def init(self, y, reinit = False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "            \n",
    "    # Compares predictions to ground truth\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "    \n",
    "# Accuracy calculation for classification model\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    \n",
    "    # No initialization is needed\n",
    "    def init (self , y):\n",
    "        pass\n",
    "    \n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare (self, predictions, y):\n",
    "        if len (y.shape) == 2:\n",
    "            y = np.argmax(y, axis = 1)\n",
    "        return predictions == y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to Layers\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Layers\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "#Dense Layer\n",
    "class Layer_Dense: #Completely Random Dense Layer\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1 = 0 , weight_regularizer_l2 = 0 ,\n",
    "                 bias_regularizer_l1 = 0 , bias_regularizer_l2 = 0 ):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) #initialize weights\n",
    "        #Note: Multiplied by 0.01 since it is often better to have start weights that minimally affect the training\n",
    "        self.biases = np.zeros((1, n_neurons)) # initialize biases to 0\n",
    "        #Note: initial bias for 0 is common to ensure neuron fires \n",
    "        \n",
    "        # Set regularization strength (lambdas)\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward (self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    #Backward Pass\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient on vregularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0 ] = - 1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0 :\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0 :\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0 :\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "#Dropout layer\n",
    "class Layer_Dropout:\n",
    "    \n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it for use in the binomial distribution\n",
    "        self.rate = 1 - rate\n",
    "        \n",
    "    # Forward Pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # If not in the training mode - return values\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size = inputs.shape) / self.rate\n",
    "        \n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    # Backward Pass\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000 , acc: 0.531, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.002501250625312656\n",
      "epoch: 2000 , acc: 0.580, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.0016672224074691564\n",
      "epoch: 3000 , acc: 0.536, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.0012503125781445363\n",
      "epoch: 4000 , acc: 0.602, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.0010002000400080014\n",
      "epoch: 5000 , acc: 0.610, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.0008334722453742291\n",
      "epoch: 6000 , acc: 0.612, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.000714387769681383\n",
      "epoch: 7000 , acc: 0.613, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.0006250781347668457\n",
      "epoch: 8000 , acc: 0.612, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.00055561729081009\n",
      "epoch: 9000 , acc: 0.615, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.0005000500050005\n",
      "epoch: 10000 , acc: 0.616, loss: 0.031 (data_loss: 0.031 , reg_loss: 0.000), lr: 0.00045458678061641964\n"
     ]
    }
   ],
   "source": [
    "# Regression Example\n",
    "X,y = sine_data()\n",
    "\n",
    "model = Model()\n",
    "\n",
    "#Add layers\n",
    "model.add(Layer_Dense(1,64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64,64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64,1))\n",
    "model.add(Activation_Linear())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss = Loss_MeanSquaredError(),\n",
    "    optimizer = NN.Optimizer_Adam( learning_rate = 0.005 , decay = 1e-3 ),\n",
    "    accuracy = Accuracy_Regression()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "# Train the model\n",
    "model.train(X, y, epochs = 10000 , print_every = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000 , acc: 0.858, loss: 0.486 (data_loss: 0.407 , reg_loss: 0.079), lr: 0.04762131530072861\n",
      "validation, acc: 0.873 , loss: 0.369\n",
      "epoch: 2000 , acc: 0.850, loss: 0.485 (data_loss: 0.418 , reg_loss: 0.067), lr: 0.045456611664166556\n",
      "validation, acc: 0.863 , loss: 0.350\n",
      "epoch: 3000 , acc: 0.853, loss: 0.503 (data_loss: 0.445 , reg_loss: 0.058), lr: 0.043480151310926564\n",
      "validation, acc: 0.883 , loss: 0.338\n",
      "epoch: 4000 , acc: 0.860, loss: 0.449 (data_loss: 0.389 , reg_loss: 0.061), lr: 0.04166840285011875\n",
      "validation, acc: 0.900 , loss: 0.331\n",
      "epoch: 5000 , acc: 0.856, loss: 0.451 (data_loss: 0.398 , reg_loss: 0.053), lr: 0.04000160006400256\n",
      "validation, acc: 0.873 , loss: 0.323\n",
      "epoch: 6000 , acc: 0.864, loss: 0.455 (data_loss: 0.399 , reg_loss: 0.055), lr: 0.03846301780837725\n",
      "validation, acc: 0.897 , loss: 0.331\n",
      "epoch: 7000 , acc: 0.871, loss: 0.428 (data_loss: 0.377 , reg_loss: 0.051), lr: 0.03703840882995667\n",
      "validation, acc: 0.873 , loss: 0.323\n",
      "epoch: 8000 , acc: 0.864, loss: 0.451 (data_loss: 0.395 , reg_loss: 0.055), lr: 0.03571556127004536\n",
      "validation, acc: 0.880 , loss: 0.308\n",
      "epoch: 9000 , acc: 0.877, loss: 0.422 (data_loss: 0.373 , reg_loss: 0.049), lr: 0.034483947722335255\n",
      "validation, acc: 0.893 , loss: 0.308\n",
      "epoch: 10000 , acc: 0.858, loss: 0.447 (data_loss: 0.394 , reg_loss: 0.054), lr: 0.03333444448148271\n",
      "validation, acc: 0.880 , loss: 0.331\n"
     ]
    }
   ],
   "source": [
    "# Classification Example\n",
    "X, y = spiral_data( samples = 1000 , classes = 3 )\n",
    "X_test, y_test = spiral_data( samples = 100 , classes = 3 )\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "\n",
    "# Add layers\n",
    "model.add(Layer_Dense( 2 , 512 , weight_regularizer_l2 = 5e-4 ,\n",
    "bias_regularizer_l2 = 5e-4 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dropout( 0.1 ))\n",
    "model.add(Layer_Dense( 512 , 3 ))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss = Loss_CategoricalCrossentropy(),\n",
    "    optimizer = NN.Optimizer_Adam( learning_rate = 0.05 , decay = 5e-5 ),\n",
    "    accuracy = Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# Train the model\n",
    "model.train(X, y, validation_data = (X_test, y_test),\n",
    "            epochs = 10000 , print_every = 1000 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
