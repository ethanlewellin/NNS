{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data, spiral_data\n",
    "import random\n",
    "import requests\n",
    "from NNS import NeuralNetwork as NN #import neural net code from github to reduce copy/pasting\n",
    "from NNS import load_MNIST_Data\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models and Their Paraneters\n",
    "\n",
    "## Retrieving Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to Dense layer\n",
    "def get_parameters(self):\n",
    "    return self.weights, self.biases\n",
    "\n",
    "# Add this to Model class\n",
    "# Retrieves and returns parameters of trainable layers\n",
    "def get_parameters (self):\n",
    "    \n",
    "    # Create a list for parameters\n",
    "    parameters = []\n",
    "    \n",
    "    # Iterable trainable layers and get their parameters\n",
    "    for layer in self.trainable_layers:\n",
    "        parameters.append(layer.get_parameters())\n",
    "        \n",
    "    # Return a list\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to Dense Layer\n",
    "\n",
    "# Set weights and biases in a layer instance\n",
    "def set_parameters(self, weights, biases):\n",
    "    self.weights = weights\n",
    "    self.biases = biases\n",
    "    \n",
    "    \n",
    "# Add this to the Model Class\n",
    "  \n",
    "# Updates the model with new parameters\n",
    "def set_parameters(self, parameters):\n",
    "    # Iterate over the parameters and layers\n",
    "    # and update each layers with each set of the parameters\n",
    "    for parameter_set, layer in zip(parameters, self.trainable_layers):\n",
    "        layer.set_parameters( * parameter_set)\n",
    "        \n",
    "# Update Model Finalize function to account for not needing an optimizer\n",
    "def finalize(self):\n",
    "    \"\"\"\"\"\"\n",
    "    # Update loss object with trainable layers\n",
    "    if self.loss is not None:\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "        \n",
    "# Update Model Set function to account for taking in paramters\n",
    "# Set loss, optimizer and accuracy\n",
    "def set(self , *, loss = None, optimizer = None, accuracy = None):\n",
    "    \n",
    "    if loss is not None:\n",
    "        self.loss = loss\n",
    "    \n",
    "    if optimizer is not None :\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    if accuracy is not None :\n",
    "        self.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # For serializing  nidek ivject\n",
    "\n",
    "# Add save parameters function to model\n",
    "# Saves the parameters to a file\n",
    "def save_parameters(self, path):\n",
    "    \n",
    "    # Open a file in the binary-write mode and save parameters to it\n",
    "    with open (path, 'wb') as f:\n",
    "        pickle.dump(self.get_parameters(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights and updates a model instance with them\n",
    "def load_parameters(self, path):\n",
    "    # Open file in the binary-read mode,\n",
    "    # load weights and update trainable layers\n",
    "    with open (path, 'rb' ) as f:\n",
    "        self.set_parameters(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the model\n",
    "import copy\n",
    "\n",
    "def save(self, path):\n",
    "    \n",
    "    # Make a deep copy of current model instance\n",
    "    model = copy.deepcopy(self)\n",
    "    \n",
    "    # Reset accumulated values in loss and accuracy objects\n",
    "    model.loss.new_pass()\n",
    "    model.accuracy.new_pass()\n",
    "    \n",
    "    # Remove data from the input layer and gradients from the loss object\n",
    "    model.input_layer.__dict__.pop('output' , None)\n",
    "    model.loss.__dict__.pop('dinputs', None)\n",
    "    \n",
    "    # For each layer remove inputs, output and dinputs properties\n",
    "    for layer in model.layers:\n",
    "        for property in [ 'inputs', 'output', 'dinputs', 'dweights', 'dbiases']:\n",
    "            layer.__dict__.pop( property , None )\n",
    "    \n",
    "    # Open a file in the binary-write mode and save the model\n",
    "    with open (path, 'wb' ) as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads and returns a model\n",
    "@staticmethod #create a model object without first needing to instantiate a model object\n",
    "def load(path):\n",
    "    \n",
    "    # Open file in the binary-read mode, load a model\n",
    "    with open (path, 'rb' ) as f:\n",
    "        model = pickle.load(f)\n",
    "        \n",
    "    # Return a model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'\n",
    "\n",
    "#load_MNIST_Data.download_mnist_dataset(URL = URL, FILE = FILE, FOLDER = FOLDER)\n",
    "X, y, X_test, y_test = load_MNIST_Data.create_data_mnist('fashion_mnist_images' )\n",
    "\n",
    "# Shuffle the training dataset\n",
    "keys = np.array( range (X.shape[ 0 ]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0 , acc: 0.109, loss: 2.303 (data_loss: 2.303 , reg_loss: 0.000), lr: 0.001\n",
      "step: 100 , acc: 0.648, loss: 0.781 (data_loss: 0.781 , reg_loss: 0.000), lr: 0.0009090909090909091\n",
      "step: 200 , acc: 0.812, loss: 0.484 (data_loss: 0.484 , reg_loss: 0.000), lr: 0.0008333333333333334\n",
      "step: 300 , acc: 0.820, loss: 0.486 (data_loss: 0.486 , reg_loss: 0.000), lr: 0.0007692307692307692\n",
      "step: 400 , acc: 0.820, loss: 0.539 (data_loss: 0.539 , reg_loss: 0.000), lr: 0.0007142857142857143\n",
      "step: 468 , acc: 0.812, loss: 0.457 (data_loss: 0.457 , reg_loss: 0.000), lr: 0.000681198910081744\n",
      "training, acc: 0.757, loss: 0.659 (data_loss: 0.659, reg_loss: 0.000 ), lr: 0.000681198910081744\n",
      "validation, acc: 0.824,loss: 0.487\n",
      "epoch: 2\n",
      "step: 0 , acc: 0.859, loss: 0.453 (data_loss: 0.453 , reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "step: 100 , acc: 0.781, loss: 0.523 (data_loss: 0.523 , reg_loss: 0.000), lr: 0.0006373486297004461\n",
      "step: 200 , acc: 0.883, loss: 0.335 (data_loss: 0.335 , reg_loss: 0.000), lr: 0.0005991611743559018\n",
      "step: 300 , acc: 0.844, loss: 0.399 (data_loss: 0.399 , reg_loss: 0.000), lr: 0.0005652911249293386\n",
      "step: 400 , acc: 0.812, loss: 0.506 (data_loss: 0.506 , reg_loss: 0.000), lr: 0.0005350454788657037\n",
      "step: 468 , acc: 0.823, loss: 0.419 (data_loss: 0.419 , reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "training, acc: 0.844, loss: 0.428 (data_loss: 0.428, reg_loss: 0.000 ), lr: 0.0005162622612287042\n",
      "validation, acc: 0.838,loss: 0.456\n",
      "epoch: 3\n",
      "step: 0 , acc: 0.859, loss: 0.448 (data_loss: 0.448 , reg_loss: 0.000), lr: 0.0005159958720330237\n",
      "step: 100 , acc: 0.797, loss: 0.514 (data_loss: 0.514 , reg_loss: 0.000), lr: 0.0004906771344455348\n",
      "step: 200 , acc: 0.891, loss: 0.294 (data_loss: 0.294 , reg_loss: 0.000), lr: 0.0004677268475210477\n",
      "step: 300 , acc: 0.867, loss: 0.365 (data_loss: 0.365 , reg_loss: 0.000), lr: 0.00044682752457551384\n",
      "step: 400 , acc: 0.805, loss: 0.503 (data_loss: 0.503 , reg_loss: 0.000), lr: 0.00042771599657827206\n",
      "step: 468 , acc: 0.812, loss: 0.408 (data_loss: 0.408 , reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "training, acc: 0.860, loss: 0.384 (data_loss: 0.384, reg_loss: 0.000 ), lr: 0.0004156275976724854\n",
      "validation, acc: 0.849,loss: 0.421\n",
      "epoch: 4\n",
      "step: 0 , acc: 0.859, loss: 0.426 (data_loss: 0.426 , reg_loss: 0.000), lr: 0.0004154549231408392\n",
      "step: 100 , acc: 0.797, loss: 0.504 (data_loss: 0.504 , reg_loss: 0.000), lr: 0.00039888312724371757\n",
      "step: 200 , acc: 0.891, loss: 0.277 (data_loss: 0.277 , reg_loss: 0.000), lr: 0.0003835826620636747\n",
      "step: 300 , acc: 0.883, loss: 0.344 (data_loss: 0.344 , reg_loss: 0.000), lr: 0.0003694126339120798\n",
      "step: 400 , acc: 0.820, loss: 0.480 (data_loss: 0.480 , reg_loss: 0.000), lr: 0.0003562522265764161\n",
      "step: 468 , acc: 0.802, loss: 0.403 (data_loss: 0.403 , reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "training, acc: 0.869, loss: 0.358 (data_loss: 0.358, reg_loss: 0.000 ), lr: 0.00034782608695652176\n",
      "validation, acc: 0.857,loss: 0.401\n",
      "epoch: 5\n",
      "step: 0 , acc: 0.859, loss: 0.418 (data_loss: 0.418 , reg_loss: 0.000), lr: 0.0003477051460361613\n",
      "step: 100 , acc: 0.789, loss: 0.477 (data_loss: 0.477 , reg_loss: 0.000), lr: 0.00033602150537634406\n",
      "step: 200 , acc: 0.891, loss: 0.267 (data_loss: 0.267 , reg_loss: 0.000), lr: 0.00032509752925877764\n",
      "step: 300 , acc: 0.883, loss: 0.326 (data_loss: 0.326 , reg_loss: 0.000), lr: 0.00031486146095717883\n",
      "step: 400 , acc: 0.836, loss: 0.449 (data_loss: 0.449 , reg_loss: 0.000), lr: 0.00030525030525030525\n",
      "step: 468 , acc: 0.833, loss: 0.390 (data_loss: 0.390 , reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "training, acc: 0.876, loss: 0.340 (data_loss: 0.340, reg_loss: 0.000 ), lr: 0.0002990430622009569\n",
      "validation, acc: 0.862,loss: 0.388\n",
      "epoch: 6\n",
      "step: 0 , acc: 0.867, loss: 0.411 (data_loss: 0.411 , reg_loss: 0.000), lr: 0.0002989536621823617\n",
      "step: 100 , acc: 0.805, loss: 0.441 (data_loss: 0.441 , reg_loss: 0.000), lr: 0.00029027576197387516\n",
      "step: 200 , acc: 0.898, loss: 0.259 (data_loss: 0.259 , reg_loss: 0.000), lr: 0.0002820874471086037\n",
      "step: 300 , acc: 0.898, loss: 0.314 (data_loss: 0.314 , reg_loss: 0.000), lr: 0.00027434842249657066\n",
      "step: 400 , acc: 0.844, loss: 0.428 (data_loss: 0.428 , reg_loss: 0.000), lr: 0.000267022696929239\n",
      "step: 468 , acc: 0.833, loss: 0.376 (data_loss: 0.376 , reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "training, acc: 0.881, loss: 0.327 (data_loss: 0.327, reg_loss: 0.000 ), lr: 0.00026226068712300026\n",
      "validation, acc: 0.864,loss: 0.379\n",
      "epoch: 7\n",
      "step: 0 , acc: 0.859, loss: 0.399 (data_loss: 0.399 , reg_loss: 0.000), lr: 0.00026219192448872575\n",
      "step: 100 , acc: 0.820, loss: 0.410 (data_loss: 0.410 , reg_loss: 0.000), lr: 0.00025549310168625444\n",
      "step: 200 , acc: 0.906, loss: 0.253 (data_loss: 0.253 , reg_loss: 0.000), lr: 0.00024912805181863477\n",
      "step: 300 , acc: 0.906, loss: 0.304 (data_loss: 0.304 , reg_loss: 0.000), lr: 0.0002430724355858046\n",
      "step: 400 , acc: 0.867, loss: 0.414 (data_loss: 0.414 , reg_loss: 0.000), lr: 0.00023730422401518745\n",
      "step: 468 , acc: 0.833, loss: 0.367 (data_loss: 0.367 , reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "training, acc: 0.884, loss: 0.316 (data_loss: 0.316, reg_loss: 0.000 ), lr: 0.00023353573096683791\n",
      "validation, acc: 0.866,loss: 0.371\n",
      "epoch: 8\n",
      "step: 0 , acc: 0.867, loss: 0.398 (data_loss: 0.398 , reg_loss: 0.000), lr: 0.00023348120476301658\n",
      "step: 100 , acc: 0.836, loss: 0.388 (data_loss: 0.388 , reg_loss: 0.000), lr: 0.00022815423226100847\n",
      "step: 200 , acc: 0.906, loss: 0.249 (data_loss: 0.249 , reg_loss: 0.000), lr: 0.0002230649118893598\n",
      "step: 300 , acc: 0.898, loss: 0.298 (data_loss: 0.298 , reg_loss: 0.000), lr: 0.00021819768710451667\n",
      "step: 400 , acc: 0.875, loss: 0.404 (data_loss: 0.404 , reg_loss: 0.000), lr: 0.00021353833013025838\n",
      "step: 468 , acc: 0.833, loss: 0.358 (data_loss: 0.358 , reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "training, acc: 0.888, loss: 0.307 (data_loss: 0.307, reg_loss: 0.000 ), lr: 0.00021048200378867611\n",
      "validation, acc: 0.868,loss: 0.366\n",
      "epoch: 9\n",
      "step: 0 , acc: 0.867, loss: 0.391 (data_loss: 0.391 , reg_loss: 0.000), lr: 0.0002104377104377104\n",
      "step: 100 , acc: 0.828, loss: 0.374 (data_loss: 0.374 , reg_loss: 0.000), lr: 0.0002061005770816158\n",
      "step: 200 , acc: 0.906, loss: 0.243 (data_loss: 0.243 , reg_loss: 0.000), lr: 0.00020193861066235866\n",
      "step: 300 , acc: 0.898, loss: 0.289 (data_loss: 0.289 , reg_loss: 0.000), lr: 0.0001979414093428345\n",
      "step: 400 , acc: 0.883, loss: 0.396 (data_loss: 0.396 , reg_loss: 0.000), lr: 0.0001940993788819876\n",
      "step: 468 , acc: 0.833, loss: 0.348 (data_loss: 0.348 , reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "training, acc: 0.890, loss: 0.300 (data_loss: 0.300, reg_loss: 0.000 ), lr: 0.00019157088122605365\n",
      "validation, acc: 0.870,loss: 0.362\n",
      "epoch: 10\n",
      "step: 0 , acc: 0.867, loss: 0.386 (data_loss: 0.386 , reg_loss: 0.000), lr: 0.0001915341888527102\n",
      "step: 100 , acc: 0.844, loss: 0.360 (data_loss: 0.360 , reg_loss: 0.000), lr: 0.00018793459875963167\n",
      "step: 200 , acc: 0.906, loss: 0.236 (data_loss: 0.236 , reg_loss: 0.000), lr: 0.00018446781036709093\n",
      "step: 300 , acc: 0.898, loss: 0.281 (data_loss: 0.281 , reg_loss: 0.000), lr: 0.00018112660749864155\n",
      "step: 400 , acc: 0.883, loss: 0.387 (data_loss: 0.387 , reg_loss: 0.000), lr: 0.00017790428749332856\n",
      "step: 468 , acc: 0.833, loss: 0.340 (data_loss: 0.340 , reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "training, acc: 0.893, loss: 0.293 (data_loss: 0.293, reg_loss: 0.000 ), lr: 0.00017577781683951485\n",
      "validation, acc: 0.872,loss: 0.359\n",
      "Evaluate testing data: \n",
      "validation, acc: 0.872,loss: 0.359\n",
      "Evaluate training data: \n",
      "validation, acc: 0.896,loss: 0.286\n"
     ]
    }
   ],
   "source": [
    "#load_MNIST_Data.download_mnist_dataset(URL = URL, FILE = FILE, FOLDER = FOLDER)\n",
    "X, y, X_test, y_test = load_MNIST_Data.create_data_mnist( 'fashion_mnist_images' )\n",
    "\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5\n",
    "# Instantiate the model\n",
    "model = NN.Model()\n",
    "# Add layers\n",
    "model.add(NN.Layer_Dense(X.shape[ 1 ], 128 ))\n",
    "model.add(NN.Activation_ReLU())\n",
    "model.add(NN.Layer_Dense( 128 , 128 ))\n",
    "model.add(NN.Activation_ReLU())\n",
    "model.add(NN.Layer_Dense( 128 , 10 ))\n",
    "model.add(NN.Activation_Softmax())\n",
    "model.set(\n",
    "loss = NN.Loss_CategoricalCrossentropy(),\n",
    "    optimizer = NN.Optimizer_Adam( decay = 1e-3 ),\n",
    "    accuracy = NN.Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "# Train the model\n",
    "model.train(X, y, validation_data = (X_test, y_test),\n",
    "            epochs = 10 , batch_size = 128 , print_every = 100 )\n",
    "\n",
    "print(\"Evaluate testing data: \")\n",
    "model.evaluate(X_test, y_test)\n",
    "print(\"Evaluate training data: \")\n",
    "model.evaluate(X, y)\n",
    "\n",
    "model.save( 'fashion_mnist.model' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.872,loss: 0.359\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = NN.Model.load( 'fashion_mnist.model' )\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
