{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data, spiral_data\n",
    "import random\n",
    "import requests\n",
    "from NNS import NeuralNetwork as NN #import neural net code from github to reduce copy/pasting\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping images...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'\n",
    "\n",
    "if not os.path.isfile(FILE):\n",
    "    print(f'Downloading {URL} and saving as {FILE} ...')\n",
    "    urllib.request.urlretrieve(URL, FILE)\n",
    "    \n",
    "print ( 'Unzipping images...' )\n",
    "with ZipFile(FILE) as zip_images:\n",
    "    zip_images.extractall(FOLDER)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "['0000.png', '0001.png', '0002.png', '0003.png', '0004.png', '0005.png', '0006.png', '0007.png', '0008.png', '0009.png']\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "#Show data information\n",
    "labels = os.listdir( 'fashion_mnist_images/train' )\n",
    "print (labels)\n",
    "\n",
    "files = os.listdir( 'fashion_mnist_images/train/0' )\n",
    "print (files[: 10 ])\n",
    "print ( len (files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  49 135 182 150  59   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  78 255 220 212 219 255 246 191 155  87   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  57 206 215 203 191 203 212 216 217 220 211  15   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0  58 231 220 210 199 209 218 218 217 208 200 215  56   0]\n",
      " [  0   0   0   0   1   2   0   0   4   0   0   0   0 145 213 207 199 187 203 210 216 217 215 215 206 215 130   0]\n",
      " [  0   0   0   0   1   2   4   0   0   0   3 105 225 205 190 201 210 214 213 215 215 212 211 208 205 207 218   0]\n",
      " [  1   5   7   0   0   0   0   0  52 162 217 189 174 157 187 198 202 217 220 223 224 222 217 211 217 201 247  65]\n",
      " [  0   0   0   0   0   0  21  72 185 189 171 171 185 203 200 207 208 209 214 219 222 222 224 215 218 211 212 148]\n",
      " [  0  70 114 129 145 159 179 196 172 176 185 196 199 206 201 210 212 213 216 218 219 217 212 207 208 200 198 173]\n",
      " [  0 122 158 184 194 192 193 196 203 209 211 211 215 218 221 222 226 227 227 226 226 223 222 216 211 208 216 185]\n",
      " [ 21   0   0  12  48  82 123 152 170 184 195 211 225 232 233 237 242 242 240 240 238 236 222 209 200 193 185 106]\n",
      " [ 26  47  54  18   5   0   0   0   0   0   0   0   0   0   2   4   6   9   9   8   9   6   6   4   2   0   0   0]\n",
      " [  0  10  27  45  55  59  57  50  44  51  58  62  65  56  54  57  59  61  60  63  68  67  66  73  77  74  65  39]\n",
      " [  0   0   0   0   4   9  18  23  26  25  23  25  29  37  38  37  39  36  29  31  33  34  28  24  20  14   7   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display data\n",
    "\n",
    "import cv2\n",
    "\n",
    "np.set_printoptions( linewidth = 200 )\n",
    "image_data = cv2.imread( 'fashion_mnist_images/train/7/0002.png', cv2.IMREAD_UNCHANGED)\n",
    "print(image_data)\n",
    "\n",
    "plt.imshow(image_data, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset , path):\n",
    "    \n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "    # Create lists for samples and labels\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # Read the image\n",
    "            image = cv2.imread(os.path.join(\n",
    "                path, dataset, label, file\n",
    "                ), cv2.IMREAD_UNCHANGED)\n",
    "            \n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "            \n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype( 'uint8' )\n",
    "\n",
    "# MNIST dataset (train + test)\n",
    "def create_data_mnist ( path ):\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset( 'train' , path)\n",
    "    X_test, y_test = load_mnist_dataset( 'test' , path)\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test\n",
    "\n",
    "#Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Range: (-1.0, 1.0)\n",
      "Data Starting Shape: (60000, 28, 28)\n",
      "Data Ending Shape: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Scale features (data is currently [0,255], we scale it to [-1,1])\n",
    "X = (X.astype(np.float32) - 127.5 ) / 127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5 ) / 127.5\n",
    "print(f\"Data Range: {X.min(), X.max()}\")\n",
    "\n",
    "# Flatten the data\n",
    "print(f\"Data Starting Shape: {X.shape}\")\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
    "print(f\"Data Ending Shape: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting y head key order: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Ending y head key order: [5 6 8 1 8 7 7 0 7 3 9 7 7 1 7 9 2 2 0 3]\n"
     ]
    }
   ],
   "source": [
    "# Our data is currently ordered by their classification, we need to shuffle it for better training\n",
    "print(f\"Starting y head key order: {y[:20]}\")\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "print(f\"Ending y head key order: {y[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Common batch sizes range between 32 and 128 samples. \n",
    "You can go smaller if there are issues fitting everything into memory \n",
    "or larger if you want faster training\n",
    "\n",
    "In general: \n",
    "\n",
    "larger batch --> increased accuracy and loss (diminishing returns) \n",
    "\n",
    "larger_batch --> slower training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to the loss, accuracy, and model classes to account for batching\n",
    " \n",
    "Common Loss Class\n",
    "class Loss:\n",
    "    \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate ( self, output, y, *, include_regularization = False):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # Add accumulated sum of losses and sample count\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len (sample_losses)\n",
    "        \n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # Calculate regulariation loss\n",
    "        # iterate all trainable layers\n",
    "        \n",
    "        for layer in self.trainable_layers:\n",
    "            \n",
    "            # L1 regularization - weights\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            \n",
    "            # L2 regularization - weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "                \n",
    "            # L1 regularization - biases\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "                \n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "            \n",
    "        return regularization_loss\n",
    "    \n",
    "    # Calculates accumulated loss\n",
    "    def calculate_accumulated ( self , * , include_regularization = False ):\n",
    "        # Calculate mean loss\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    # Set/remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "        \n",
    "    # Reset variables for accumulated loss\n",
    "    def new_pass (self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "Common Accuracy Class\n",
    "class Accuracy:\n",
    "    \n",
    "    # Calculates an accuracy\n",
    "    # given prediction and ground truth values\n",
    "    def calculate(self, predictions, y):\n",
    "        \n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "        \n",
    "        # Calculate an accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        # Add accumulated sum of matching values and sample count\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len (comparisons)\n",
    "        \n",
    "        # Return accuracy\n",
    "        return accuracy\n",
    "    \n",
    "    # Calculates accumulated accuracy\n",
    "    def calculate_accumulated ( self ):\n",
    "        \n",
    "        # Calculate an accuracy\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        \n",
    "        # Return the data and regularization losses\n",
    "        return accuracy\n",
    "    \n",
    "    # Reset variables for accumulated accuracy\n",
    "    def new_pass ( self ):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "        \n",
    "Model class\n",
    "class Model :\n",
    "    \n",
    "    def __init__ (self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    # Add objects to the model\n",
    "    def add (self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set (self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    # Finalize the model\n",
    "    def finalize (self):\n",
    "        \n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "            \n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0 :\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "                \n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1 :\n",
    "                self.layers[i].prev = self.layers[i - 1] \n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "                \n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else :\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "                # Update loss object with trainable layers\n",
    "                self.loss.remember_trainable_layers(\n",
    "                    self.trainable_layers\n",
    "                    )\n",
    "                \n",
    "                \n",
    "            # If output activation is Softmax and\n",
    "            # loss function is Categorical Cross-Entropy\n",
    "            # create an object of combined activation\n",
    "            # and loss function containing\n",
    "            # faster gradient calculation\n",
    "            if isinstance(self.layers[ - 1 ], Activation_Softmax) and \\\n",
    "                isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "                # Create an object of combined activation\n",
    "                # and loss functions\n",
    "                self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs = 1, batch_size=None, print_every = 1, validation_data = None):\n",
    "        \n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "        \n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None :\n",
    "            validation_steps = 1\n",
    "            \n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "            # Calculate number of steps\n",
    "            if batch_size is not None:\n",
    "                train_steps = len (X) // batch_size\n",
    "                \n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data, but not a full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if train_steps * batch_size < len (X):\n",
    "                    train_steps += 1\n",
    "                    \n",
    "                if validation_data is not None :\n",
    "                    validation_steps = len (X_val) // batch_size\n",
    "                    # Dividing rounds down. If there are some remaining\n",
    "                    # data, but nor full batch, this won't include it\n",
    "                    # Add `1` to include this not full batch\n",
    "                    if validation_steps * batch_size < len (X_val):\n",
    "                        validation_steps += 1\n",
    "        \n",
    "        # Main training loop\n",
    "        for epoch in range (1, epochs+1):\n",
    "\n",
    "            print (f'epoch: {epoch}')\n",
    "            \n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None :\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                    \n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size:(step + 1 ) * batch_size]\n",
    "                    batch_y = y[step * batch_size:(step + 1 ) * batch_size]\n",
    "                    \n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training = True)\n",
    "            \n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization = True)\n",
    "                loss = data_loss + regularization_loss\n",
    "            \n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "            \n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "            \n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "            \n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print ( f'epoch: {step} , ' +\n",
    "                            f'acc: {accuracy :.3f}, ' +\n",
    "                            f'loss: {loss :.3f} (' +\n",
    "                            f'data_loss: {data_loss :.3f} , ' +\n",
    "                            f'reg_loss: {regularization_loss :.3f}), ' +\n",
    "                            f'lr: {self.optimizer.current_learning_rate}')\n",
    "                    \n",
    "            # Get and print epoch loss and accuracy\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(\n",
    "                include_regularization = True)\n",
    "            \n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            \n",
    "            print ( f'training, ' +\n",
    "                    f'acc: {epoch_accuracy :.3f}, ' +\n",
    "                    f'loss: {epoch_loss :.3f} (' +\n",
    "                    f'data_loss: {epoch_data_loss :.3f}, ' +\n",
    "                    f'reg_loss: {epoch_regularization_loss :.3f} ), ' +\n",
    "                    f'lr: {self.optimizer.current_learning_rate}' )\n",
    "                \n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "                \n",
    "                # Reset accumulated values in loss\n",
    "                # and accuracy objects\n",
    "                self.loss.new_pass()\n",
    "                self.accuracy.new_pass()\n",
    "                \n",
    "                # Iterate over steps\n",
    "                for step in range(validation_steps):\n",
    "                    \n",
    "                    # If batch size is not set -\n",
    "                    # train using one step and full dataset\n",
    "                    if batch_size is None :\n",
    "                        batch_X = X_val\n",
    "                        batch_y = y_val\n",
    "                    \n",
    "                    # Otherwise slice a batch\n",
    "                    else:\n",
    "                        batch_X = X_val[step * batch_size:(step + 1 ) * batch_size]\n",
    "                        batch_y = y_val[step * batch_size:(step + 1 ) * batch_size]\n",
    "                \n",
    "                    # Perform the forward pass\n",
    "                    output = self.forward(batch_X, training=False)\n",
    "                \n",
    "                    # Calculate the loss\n",
    "                    self.loss.calculate(output, batch_y)\n",
    "                \n",
    "                    # Get predictions and calculate an accuracy\n",
    "                    predictions = self.output_layer_activation.predictions(output)\n",
    "                    self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                # Get and print validation loss and accuracy\n",
    "                validation_loss = self.loss.calculate_accumulated()\n",
    "                validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "                \n",
    "                # Print a summary\n",
    "                print ( f'validation, ' +\n",
    "                        f'acc: {validation_accuracy :.3f} , ' +\n",
    "                        f'loss: {validation_loss :.3f}')\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward ( self , X , training ):\n",
    "        \n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "    \n",
    "    # Performs backward pass\n",
    "    def backward ( self , output , y ):\n",
    "        \n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None :\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            \n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "            \n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "                \n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed (self.layers):\n",
    "            layer.backward(layer.next.dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "epoch: 0 , acc: 0.078, loss: 2.303 (data_loss: 2.303 , reg_loss: 0.000), lr: 0.001\n",
      "epoch: 100 , acc: 0.656, loss: 0.780 (data_loss: 0.780 , reg_loss: 0.000), lr: 0.0009950248756218907\n",
      "epoch: 200 , acc: 0.750, loss: 0.582 (data_loss: 0.582 , reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "epoch: 300 , acc: 0.789, loss: 0.567 (data_loss: 0.567 , reg_loss: 0.000), lr: 0.0009852216748768474\n",
      "epoch: 400 , acc: 0.742, loss: 0.575 (data_loss: 0.575 , reg_loss: 0.000), lr: 0.000980392156862745\n",
      "epoch: 468 , acc: 0.823, loss: 0.379 (data_loss: 0.379 , reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.728, loss: 0.728 (data_loss: 0.728, reg_loss: 0.000 ), lr: 0.0009771350400625367\n",
      "validation, acc: 0.814 , loss: 0.509\n",
      "epoch: 2\n",
      "epoch: 0 , acc: 0.805, loss: 0.582 (data_loss: 0.582 , reg_loss: 0.000), lr: 0.0009770873027505008\n",
      "epoch: 100 , acc: 0.797, loss: 0.455 (data_loss: 0.455 , reg_loss: 0.000), lr: 0.000972337012008362\n",
      "epoch: 200 , acc: 0.828, loss: 0.468 (data_loss: 0.468 , reg_loss: 0.000), lr: 0.0009676326866321544\n",
      "epoch: 300 , acc: 0.812, loss: 0.481 (data_loss: 0.481 , reg_loss: 0.000), lr: 0.0009629736626703259\n",
      "epoch: 400 , acc: 0.789, loss: 0.454 (data_loss: 0.454 , reg_loss: 0.000), lr: 0.0009583592888974076\n",
      "epoch: 468 , acc: 0.896, loss: 0.314 (data_loss: 0.314 , reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "training, acc: 0.836, loss: 0.452 (data_loss: 0.452, reg_loss: 0.000 ), lr: 0.0009552466924583273\n",
      "validation, acc: 0.835 , loss: 0.448\n",
      "epoch: 3\n",
      "epoch: 0 , acc: 0.867, loss: 0.485 (data_loss: 0.485 , reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "epoch: 100 , acc: 0.852, loss: 0.355 (data_loss: 0.355 , reg_loss: 0.000), lr: 0.0009506607091928891\n",
      "epoch: 200 , acc: 0.852, loss: 0.412 (data_loss: 0.412 , reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "epoch: 300 , acc: 0.797, loss: 0.438 (data_loss: 0.438 , reg_loss: 0.000), lr: 0.0009417082587814295\n",
      "epoch: 400 , acc: 0.805, loss: 0.421 (data_loss: 0.421 , reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "epoch: 468 , acc: 0.875, loss: 0.281 (data_loss: 0.281 , reg_loss: 0.000), lr: 0.000934317481080071\n",
      "training, acc: 0.854, loss: 0.401 (data_loss: 0.401, reg_loss: 0.000 ), lr: 0.000934317481080071\n",
      "validation, acc: 0.841 , loss: 0.431\n",
      "epoch: 4\n",
      "epoch: 0 , acc: 0.883, loss: 0.456 (data_loss: 0.456 , reg_loss: 0.000), lr: 0.0009342738356612324\n",
      "epoch: 100 , acc: 0.859, loss: 0.316 (data_loss: 0.316 , reg_loss: 0.000), lr: 0.0009299297903008323\n",
      "epoch: 200 , acc: 0.859, loss: 0.370 (data_loss: 0.370 , reg_loss: 0.000), lr: 0.0009256259545517657\n",
      "epoch: 300 , acc: 0.852, loss: 0.408 (data_loss: 0.408 , reg_loss: 0.000), lr: 0.0009213617727000506\n",
      "epoch: 400 , acc: 0.820, loss: 0.405 (data_loss: 0.405 , reg_loss: 0.000), lr: 0.0009171366992250195\n",
      "epoch: 468 , acc: 0.854, loss: 0.264 (data_loss: 0.264 , reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "training, acc: 0.864, loss: 0.372 (data_loss: 0.372, reg_loss: 0.000 ), lr: 0.0009142857142857143\n",
      "validation, acc: 0.850 , loss: 0.410\n",
      "epoch: 5\n",
      "epoch: 0 , acc: 0.875, loss: 0.419 (data_loss: 0.419 , reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "epoch: 100 , acc: 0.898, loss: 0.290 (data_loss: 0.290 , reg_loss: 0.000), lr: 0.0009100837277029487\n",
      "epoch: 200 , acc: 0.875, loss: 0.343 (data_loss: 0.343 , reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "epoch: 300 , acc: 0.859, loss: 0.386 (data_loss: 0.386 , reg_loss: 0.000), lr: 0.0009018759018759019\n",
      "epoch: 400 , acc: 0.844, loss: 0.398 (data_loss: 0.398 , reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "epoch: 468 , acc: 0.885, loss: 0.242 (data_loss: 0.242 , reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "training, acc: 0.872, loss: 0.353 (data_loss: 0.353, reg_loss: 0.000 ), lr: 0.0008950948800572861\n",
      "validation, acc: 0.855 , loss: 0.395\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = NN.Model()\n",
    "# Add layers\n",
    "model.add(NN.Layer_Dense(X.shape[1], 64))\n",
    "model.add(NN.Activation_ReLU())\n",
    "model.add(NN.Layer_Dense(64, 64))\n",
    "model.add(NN.Activation_ReLU())\n",
    "model.add(NN.Layer_Dense(64, 10))\n",
    "model.add(NN.Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss = NN.Loss_CategoricalCrossentropy(),\n",
    "    optimizer = NN.Optimizer_Adam(decay = 5e-5),\n",
    "    accuracy = NN.Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# Train the model\n",
    "model.train(X, y, validation_data = (X_test, y_test), epochs = 5 , batch_size = 128 , print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "epoch: 0 , acc: 0.078, loss: 2.303 (data_loss: 2.303 , reg_loss: 0.000), lr: 0.001\n",
      "epoch: 100 , acc: 0.672, loss: 0.753 (data_loss: 0.753 , reg_loss: 0.000), lr: 0.0009090909090909091\n",
      "epoch: 200 , acc: 0.773, loss: 0.555 (data_loss: 0.555 , reg_loss: 0.000), lr: 0.0008333333333333334\n",
      "epoch: 300 , acc: 0.797, loss: 0.543 (data_loss: 0.543 , reg_loss: 0.000), lr: 0.0007692307692307692\n",
      "epoch: 400 , acc: 0.773, loss: 0.499 (data_loss: 0.499 , reg_loss: 0.000), lr: 0.0007142857142857143\n",
      "epoch: 468 , acc: 0.833, loss: 0.343 (data_loss: 0.343 , reg_loss: 0.000), lr: 0.000681198910081744\n",
      "training, acc: 0.759, loss: 0.652 (data_loss: 0.652, reg_loss: 0.000 ), lr: 0.000681198910081744\n",
      "validation, acc: 0.822 , loss: 0.476\n",
      "epoch: 2\n",
      "epoch: 0 , acc: 0.812, loss: 0.552 (data_loss: 0.552 , reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "epoch: 100 , acc: 0.812, loss: 0.428 (data_loss: 0.428 , reg_loss: 0.000), lr: 0.0006373486297004461\n",
      "epoch: 200 , acc: 0.844, loss: 0.427 (data_loss: 0.427 , reg_loss: 0.000), lr: 0.0005991611743559018\n",
      "epoch: 300 , acc: 0.828, loss: 0.454 (data_loss: 0.454 , reg_loss: 0.000), lr: 0.0005652911249293386\n",
      "epoch: 400 , acc: 0.812, loss: 0.424 (data_loss: 0.424 , reg_loss: 0.000), lr: 0.0005350454788657037\n",
      "epoch: 468 , acc: 0.875, loss: 0.306 (data_loss: 0.306 , reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "training, acc: 0.845, loss: 0.426 (data_loss: 0.426, reg_loss: 0.000 ), lr: 0.0005162622612287042\n",
      "validation, acc: 0.841 , loss: 0.433\n",
      "epoch: 3\n",
      "epoch: 0 , acc: 0.859, loss: 0.487 (data_loss: 0.487 , reg_loss: 0.000), lr: 0.0005159958720330237\n",
      "epoch: 100 , acc: 0.859, loss: 0.340 (data_loss: 0.340 , reg_loss: 0.000), lr: 0.0004906771344455348\n",
      "epoch: 200 , acc: 0.852, loss: 0.366 (data_loss: 0.366 , reg_loss: 0.000), lr: 0.0004677268475210477\n",
      "epoch: 300 , acc: 0.844, loss: 0.402 (data_loss: 0.402 , reg_loss: 0.000), lr: 0.00044682752457551384\n",
      "epoch: 400 , acc: 0.812, loss: 0.406 (data_loss: 0.406 , reg_loss: 0.000), lr: 0.00042771599657827206\n",
      "epoch: 468 , acc: 0.865, loss: 0.302 (data_loss: 0.302 , reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "training, acc: 0.858, loss: 0.387 (data_loss: 0.387, reg_loss: 0.000 ), lr: 0.0004156275976724854\n",
      "validation, acc: 0.853 , loss: 0.410\n",
      "epoch: 4\n",
      "epoch: 0 , acc: 0.875, loss: 0.447 (data_loss: 0.447 , reg_loss: 0.000), lr: 0.0004154549231408392\n",
      "epoch: 100 , acc: 0.859, loss: 0.319 (data_loss: 0.319 , reg_loss: 0.000), lr: 0.00039888312724371757\n",
      "epoch: 200 , acc: 0.867, loss: 0.334 (data_loss: 0.334 , reg_loss: 0.000), lr: 0.0003835826620636747\n",
      "epoch: 300 , acc: 0.852, loss: 0.372 (data_loss: 0.372 , reg_loss: 0.000), lr: 0.0003694126339120798\n",
      "epoch: 400 , acc: 0.828, loss: 0.381 (data_loss: 0.381 , reg_loss: 0.000), lr: 0.0003562522265764161\n",
      "epoch: 468 , acc: 0.875, loss: 0.291 (data_loss: 0.291 , reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "training, acc: 0.867, loss: 0.364 (data_loss: 0.364, reg_loss: 0.000 ), lr: 0.00034782608695652176\n",
      "validation, acc: 0.858 , loss: 0.398\n",
      "epoch: 5\n",
      "epoch: 0 , acc: 0.883, loss: 0.419 (data_loss: 0.419 , reg_loss: 0.000), lr: 0.0003477051460361613\n",
      "epoch: 100 , acc: 0.867, loss: 0.298 (data_loss: 0.298 , reg_loss: 0.000), lr: 0.00033602150537634406\n",
      "epoch: 200 , acc: 0.859, loss: 0.316 (data_loss: 0.316 , reg_loss: 0.000), lr: 0.00032509752925877764\n",
      "epoch: 300 , acc: 0.867, loss: 0.359 (data_loss: 0.359 , reg_loss: 0.000), lr: 0.00031486146095717883\n",
      "epoch: 400 , acc: 0.820, loss: 0.362 (data_loss: 0.362 , reg_loss: 0.000), lr: 0.00030525030525030525\n",
      "epoch: 468 , acc: 0.875, loss: 0.276 (data_loss: 0.276 , reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "training, acc: 0.874, loss: 0.347 (data_loss: 0.347, reg_loss: 0.000 ), lr: 0.0002990430622009569\n",
      "validation, acc: 0.863 , loss: 0.389\n",
      "epoch: 6\n",
      "epoch: 0 , acc: 0.891, loss: 0.397 (data_loss: 0.397 , reg_loss: 0.000), lr: 0.0002989536621823617\n",
      "epoch: 100 , acc: 0.875, loss: 0.279 (data_loss: 0.279 , reg_loss: 0.000), lr: 0.00029027576197387516\n",
      "epoch: 200 , acc: 0.867, loss: 0.305 (data_loss: 0.305 , reg_loss: 0.000), lr: 0.0002820874471086037\n",
      "epoch: 300 , acc: 0.867, loss: 0.349 (data_loss: 0.349 , reg_loss: 0.000), lr: 0.00027434842249657066\n",
      "epoch: 400 , acc: 0.844, loss: 0.350 (data_loss: 0.350 , reg_loss: 0.000), lr: 0.000267022696929239\n",
      "epoch: 468 , acc: 0.885, loss: 0.263 (data_loss: 0.263 , reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "training, acc: 0.879, loss: 0.333 (data_loss: 0.333, reg_loss: 0.000 ), lr: 0.00026226068712300026\n",
      "validation, acc: 0.864 , loss: 0.381\n",
      "epoch: 7\n",
      "epoch: 0 , acc: 0.891, loss: 0.383 (data_loss: 0.383 , reg_loss: 0.000), lr: 0.00026219192448872575\n",
      "epoch: 100 , acc: 0.883, loss: 0.263 (data_loss: 0.263 , reg_loss: 0.000), lr: 0.00025549310168625444\n",
      "epoch: 200 , acc: 0.867, loss: 0.296 (data_loss: 0.296 , reg_loss: 0.000), lr: 0.00024912805181863477\n",
      "epoch: 300 , acc: 0.867, loss: 0.341 (data_loss: 0.341 , reg_loss: 0.000), lr: 0.0002430724355858046\n",
      "epoch: 400 , acc: 0.859, loss: 0.342 (data_loss: 0.342 , reg_loss: 0.000), lr: 0.00023730422401518745\n",
      "epoch: 468 , acc: 0.896, loss: 0.253 (data_loss: 0.253 , reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "training, acc: 0.882, loss: 0.323 (data_loss: 0.323, reg_loss: 0.000 ), lr: 0.00023353573096683791\n",
      "validation, acc: 0.867 , loss: 0.374\n",
      "epoch: 8\n",
      "epoch: 0 , acc: 0.891, loss: 0.369 (data_loss: 0.369 , reg_loss: 0.000), lr: 0.00023348120476301658\n",
      "epoch: 100 , acc: 0.891, loss: 0.251 (data_loss: 0.251 , reg_loss: 0.000), lr: 0.00022815423226100847\n",
      "epoch: 200 , acc: 0.875, loss: 0.289 (data_loss: 0.289 , reg_loss: 0.000), lr: 0.0002230649118893598\n",
      "epoch: 300 , acc: 0.867, loss: 0.332 (data_loss: 0.332 , reg_loss: 0.000), lr: 0.00021819768710451667\n",
      "epoch: 400 , acc: 0.859, loss: 0.334 (data_loss: 0.334 , reg_loss: 0.000), lr: 0.00021353833013025838\n",
      "epoch: 468 , acc: 0.896, loss: 0.241 (data_loss: 0.241 , reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "training, acc: 0.886, loss: 0.314 (data_loss: 0.314, reg_loss: 0.000 ), lr: 0.00021048200378867611\n",
      "validation, acc: 0.870 , loss: 0.368\n",
      "epoch: 9\n",
      "epoch: 0 , acc: 0.898, loss: 0.361 (data_loss: 0.361 , reg_loss: 0.000), lr: 0.0002104377104377104\n",
      "epoch: 100 , acc: 0.898, loss: 0.239 (data_loss: 0.239 , reg_loss: 0.000), lr: 0.0002061005770816158\n",
      "epoch: 200 , acc: 0.875, loss: 0.284 (data_loss: 0.284 , reg_loss: 0.000), lr: 0.00020193861066235866\n",
      "epoch: 300 , acc: 0.875, loss: 0.326 (data_loss: 0.326 , reg_loss: 0.000), lr: 0.0001979414093428345\n",
      "epoch: 400 , acc: 0.859, loss: 0.328 (data_loss: 0.328 , reg_loss: 0.000), lr: 0.0001940993788819876\n",
      "epoch: 468 , acc: 0.896, loss: 0.231 (data_loss: 0.231 , reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "training, acc: 0.888, loss: 0.306 (data_loss: 0.306, reg_loss: 0.000 ), lr: 0.00019157088122605365\n",
      "validation, acc: 0.871 , loss: 0.363\n",
      "epoch: 10\n",
      "epoch: 0 , acc: 0.891, loss: 0.354 (data_loss: 0.354 , reg_loss: 0.000), lr: 0.0001915341888527102\n",
      "epoch: 100 , acc: 0.914, loss: 0.233 (data_loss: 0.233 , reg_loss: 0.000), lr: 0.00018793459875963167\n",
      "epoch: 200 , acc: 0.875, loss: 0.280 (data_loss: 0.280 , reg_loss: 0.000), lr: 0.00018446781036709093\n",
      "epoch: 300 , acc: 0.875, loss: 0.318 (data_loss: 0.318 , reg_loss: 0.000), lr: 0.00018112660749864155\n",
      "epoch: 400 , acc: 0.867, loss: 0.320 (data_loss: 0.320 , reg_loss: 0.000), lr: 0.00017790428749332856\n",
      "epoch: 468 , acc: 0.927, loss: 0.220 (data_loss: 0.220 , reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "training, acc: 0.891, loss: 0.300 (data_loss: 0.300, reg_loss: 0.000 ), lr: 0.00017577781683951485\n",
      "validation, acc: 0.872 , loss: 0.359\n"
     ]
    }
   ],
   "source": [
    "# A larger model\n",
    "# Instantiate the model\n",
    "model = NN.Model()\n",
    "# Add layers\n",
    "model.add(NN.Layer_Dense(X.shape[1], 128))\n",
    "model.add(NN.Activation_ReLU())\n",
    "model.add(NN.Layer_Dense(128, 128))\n",
    "model.add(NN.Activation_ReLU())\n",
    "model.add(NN.Layer_Dense(128, 10))\n",
    "model.add(NN.Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss = NN.Loss_CategoricalCrossentropy(),\n",
    "    optimizer = NN.Optimizer_Adam(decay = 1e-3),\n",
    "    accuracy = NN.Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# Train the model\n",
    "model.train(X, y, validation_data = (X_test, y_test), epochs = 10 , batch_size = 128 , print_every = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
