{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from NNS import NeuralNetwork as NN #import neural net code from github to reduce copy/pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0 1.0 1.0 1.0\n",
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "x = [ 1.0 , - 2.0 , 3.0 ] # input values\n",
    "w = [ - 3.0 , - 1.0 , 2.0 ] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[ 0 ] * w[ 0 ]\n",
    "xw1 = x[ 1 ] * w[ 1 ]\n",
    "xw2 = x[ 2 ] * w[ 2 ]\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "# ReLU activation function\n",
    "y = max (z, 0 )\n",
    "\n",
    "\n",
    "# Backward pass\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * ( 1. if z > 0 else 0. )\n",
    "print (drelu_dz)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print (drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[ 0 ]\n",
    "dmul_dx1 = w[ 1 ]\n",
    "dmul_dx2 = w[ 2 ]\n",
    "dmul_dw0 = x[ 0 ]\n",
    "dmul_dw1 = x[ 1 ]\n",
    "dmul_dw2 = x[ 2 ]\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "print (drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Neuron Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "[-3.0, -1.0, 2.0] 1.0\n",
      "[-3.001, -0.998, 1.997] 0.999\n",
      "5.985\n"
     ]
    }
   ],
   "source": [
    "print(y) #Starting Weight\n",
    "\n",
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2] # gradients on inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2] # gradients on weights\n",
    "db = drelu_db # gradient on bias...just 1 bias here.\n",
    "\n",
    "print(w,b) #Current Weights and Biases\n",
    "\n",
    "#Apply Gradient\n",
    "# -0.001 Used in place of an optimizer\n",
    "w[ 0 ] += - 0.001 * dw[ 0 ]\n",
    "w[ 1 ] += - 0.001 * dw[ 1 ]\n",
    "w[ 2 ] += - 0.001 * dw[ 2 ]\n",
    "b += - 0.001 * db\n",
    "print (w, b)\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[ 0 ] * w[ 0 ]\n",
    "xw1 = x[ 1 ] * w[ 1 ]\n",
    "xw2 = x[ 2 ] * w[ 2 ]\n",
    "# Adding\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "# ReLU activation function\n",
    "y = max (z, 0 )\n",
    "print (y) #Ending Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer of Neurons Exapmle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient with Respect to Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "# Passed in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# a vector of 1s\n",
    "dvalues = np.array([[ 1. , 1. , 1. ],\n",
    "                    [ 2. , 2. , 2. ],\n",
    "                    [ 3. , 3. , 3. ]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[ 0.2 , 0.8 , - 0.5 , 1 ],\n",
    "                    [ 0.5 , - 0.91 , 0.26 , - 0.5 ],\n",
    "                    [ - 0.26 , - 0.27 , 0.17 , 0.87 ]]).T\n",
    "\n",
    "# sum weights of given input\n",
    "# and multiply by the passed in gradient for this neuron\n",
    "dinputs = np.dot(dvalues, weights.T) #gradient with respect to input\n",
    "print (dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient with Respect to Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "# Passed in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[ 1. , 1. , 1. ],\n",
    "                    [ 2. , 2. , 2. ],\n",
    "                    [ 3. , 3. , 3. ]])\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[ 1 , 2 , 3 , 2.5 ],\n",
    "                    [ 2. , 5. , - 1. , 2 ],\n",
    "                    [ - 1.5 , 2.7 , 3.3 , - 0.8 ]])\n",
    "# sum weights of given input\n",
    "# and multiply by the passed in gradient for this neuron\n",
    "dweights = np.dot(inputs.T, dvalues) #gradient with respect to weights\n",
    "print (dweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient with Respect to bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Passed in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[ 1. , 1. , 1. ],\n",
    "                    [ 2. , 2. , 2. ],\n",
    "                    [ 3. , 3. , 3. ]])\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[ 2 , 3 , 0.5 ]])\n",
    "# dbiases - sum values, do this over samples (first axis), \n",
    "# keepdims since this by default will produce a plain list\n",
    "dbiases = np.sum(dvalues, axis = 0 , keepdims = True ) #gradient with respect to bias\n",
    "print (dbiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient with Respect to Activation (ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Example layer output\n",
    "z = np.array([[ 1 , 2 , - 3 , - 4 ],\n",
    "              [ 2 , - 7 , - 1 , 3 ],\n",
    "              [ - 1 , 2 , 5 , - 1 ]])\n",
    "dvalues = np.array([[ 1 , 2 , 3 , 4 ],\n",
    "                    [ 5 , 6 , 7 , 8 ],\n",
    "                    [ 9 , 10 , 11 , 12 ]])\n",
    "\n",
    "# ReLU activation's derivative\n",
    "# with the chain rule applied\n",
    "drelu = dvalues.copy() #Don't modify original dvalues\n",
    "drelu[z <= 0 ] = 0\n",
    "print (drelu) #gradient with respect to activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward and Backward Pass of Full Layer and Batch-Based Partial Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n"
     ]
    }
   ],
   "source": [
    "# Passed in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[ 1. , 1. , 1. ],\n",
    "                    [ 2. , 2. , 2. ],\n",
    "                    [ 3. , 3. , 3. ]])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[ 1 , 2 , 3 , 2.5 ],\n",
    "                   [ 2. , 5. , - 1. , 2 ],\n",
    "                   [ - 1.5 , 2.7 , 3.3 , - 0.8 ]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[ 0.2 , 0.8 , - 0.5 , 1 ],\n",
    "                    [ 0.5 , - 0.91 , 0.26 , - 0.5 ],\n",
    "                    [ - 0.26 , - 0.27 , 0.17 , 0.87 ]]).T\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[ 2 , 3 , 0.5 ]])\n",
    "\n",
    "# Forward pass\n",
    "layer_outputs = np.dot(inputs, weights) + biases # Dense layer\n",
    "relu_outputs = np.maximum( 0 , layer_outputs) # ReLU activation\n",
    "\n",
    "# Let's optimize and test backpropagation here\n",
    "# ReLU activation - simulates derivative with respect to input values\n",
    "# from next layer passed to current layer during backpropagation\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0 ] = 0 #gradient with respect to activation function\n",
    "\n",
    "# Dense layer\n",
    "# dinputs - multiply by weights\n",
    "dinputs = np.dot(drelu, weights.T) #gradient with respect to input\n",
    "# dweights - multiply by inputs\n",
    "dweights = np.dot(inputs.T, drelu) #gradient with respect to weights\n",
    "# dbiases - sum values, do this over samples (first axis)\n",
    "# keepdims since this by default will produce a plain list\n",
    "dbiases = np.sum(drelu, axis = 0 , keepdims = True ) #gradient with respect to biases\n",
    "# Update parameters\n",
    "weights += - 0.001 * dweights\n",
    "biases += - 0.001 * dbiases\n",
    "print (weights)\n",
    "print (biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense Layer\n",
    "class Layer_Dense: #Completely Random Dense Layer\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) #initialize weights\n",
    "        #Note: Multiplied by 0.01 since it is often better to have start weights that minimally affect the training\n",
    "        self.biases = np.zeros((1, n_neurons)) # initialize biases to 0\n",
    "        #Note: initial bias for 0 is common to ensure neuron fires \n",
    "    \n",
    "    #Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    #Backward Pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "#Relu Activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "    # Backward Pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() # don't want to modify original values\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "\n",
    "#Common Loss Class\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        #calculate sample losses\n",
    "        sample_losses = self.forward(output,y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "    \n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward ( self , dvalues , y_true ):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len (dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len (dvalues[ 0 ])\n",
    "        \n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len (y_true.shape) == 1 :\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "    #Forward Pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "           \n",
    "        #Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "                    \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not affect mean\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1- 1e-7)\n",
    "        \n",
    "        # Probabilities for target values\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "                \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis = 1\n",
    "            )\n",
    "                \n",
    "        #Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    \n",
    "#Softmax Activation \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inpus = inputs\n",
    "        #Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims = True))\n",
    "        # Normalize them for each sample\n",
    "        probabilites = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilites\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        # Create uninitialized array\n",
    "        self.dinputs=np.empty_like(dvalues)\n",
    "        \n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            #Flatten output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            #Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            #Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Categorical Cross-Entropy loss and Softmax activation derivative\n",
    "\n",
    "Using both Common Categorical Cross-Entropy loss and Softmax activation results in a a much simplier, much easier to calculate partials that are faster to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy ():\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__ ( self ):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    # Forward pass\n",
    "    def forward ( self , inputs , y_true ):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    # Backward pass\n",
    "    def backward ( self , dvalues , y_true ):\n",
    "        # Number of samples\n",
    "        samples = len (dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len (y_true.shape) == 2 :\n",
    "            y_true = np.argmax(y_true, axis = 1 )\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[ range (samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: combined loss and activation:\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "Gradients: separate loss and activation:\n",
      "[[-0.09999999  0.03333334  0.06666667]\n",
      " [ 0.03333334 -0.16666667  0.13333334]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "                            [ 0.1 , 0.5 , 0.4 ],\n",
    "                            [ 0.02 , 0.9 , 0.08 ]])\n",
    "\n",
    "class_targets = np.array([0,1,1])\n",
    "\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues1 = softmax_loss.dinputs\n",
    "\n",
    "activation = Activation_Softmax()\n",
    "activation.output = softmax_outputs\n",
    "loss = Loss_CategoricalCrossentropy()\n",
    "loss.backward(softmax_outputs, class_targets)\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues2 = activation.dinputs\n",
    "\n",
    "print ( 'Gradients: combined loss and activation:' )\n",
    "print (dvalues1)\n",
    "print ( 'Gradients: separate loss and activation:' )\n",
    "print (dvalues2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.411686871161951\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "                            [ 0.1 , 0.5 , 0.4 ],\n",
    "                            [ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets = np.array([ 0 , 1 , 1 ])\n",
    "def f1 ():\n",
    "    softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "    softmax_loss.backward(softmax_outputs, class_targets)\n",
    "    dvalues1 = softmax_loss.dinputs\n",
    "def f2 ():\n",
    "    activation = Activation_Softmax()\n",
    "    activation.output = softmax_outputs\n",
    "    loss = Loss_CategoricalCrossentropy()\n",
    "    loss.backward(softmax_outputs, class_targets)\n",
    "    activation.backward(loss.dinputs)\n",
    "    dvalues2 = activation.dinputs\n",
    "    \n",
    "t1 = timeit( lambda : f1(), number = 10000 )\n",
    "t2 = timeit( lambda : f2(), number = 10000 )\n",
    "print (t2 / t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 1.0986104\n",
      "acc: 0.34\n",
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense( 2 , 3 )\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense( 3 , 3 )\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print (loss_activation.output[: 5 ])\n",
    "# Print loss value\n",
    "print ( 'loss:' , loss)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "if len (y.shape) == 2 :\n",
    "    y = np.argmax(y, axis = 1 )\n",
    "accuracy = np.mean(predictions == y)\n",
    "# Print accuracy\n",
    "print ( 'acc:' , accuracy)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "print (dense1.dweights)\n",
    "print (dense1.dbiases)\n",
    "print (dense2.dweights)\n",
    "print (dense2.dbiases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
